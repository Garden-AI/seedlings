{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ3ybAndV_SZ",
        "outputId": "f83ed247-9252-4577-ba4e-6bffa098cf5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "## The usual imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "## print out the pytorch version used\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XOJD3283WDWr"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[-1.0],  [0.0], [1.0], [2.0], [3.0], [4.0]], dtype=torch.float)\n",
        "y = torch.tensor([[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]], dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RkrOwnulWY8t"
      },
      "outputs": [],
      "source": [
        "## Neural network with 1 hidden layer\n",
        "layer1 = nn.Linear(1,1, bias=False)\n",
        "model = nn.Sequential(layer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eCZWhDqVWaw1"
      },
      "outputs": [],
      "source": [
        "## loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "## optimizer algorithm\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6e_bLoKWl43",
        "outputId": "8add6250-830c-4737-cf4e-4216c369a31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 7.1485\n",
            "Epoch: 1 | Loss: 5.8581\n",
            "Epoch: 2 | Loss: 4.8206\n",
            "Epoch: 3 | Loss: 3.9865\n",
            "Epoch: 4 | Loss: 3.3158\n",
            "Epoch: 5 | Loss: 2.7766\n",
            "Epoch: 6 | Loss: 2.3430\n",
            "Epoch: 7 | Loss: 1.9945\n",
            "Epoch: 8 | Loss: 1.7142\n",
            "Epoch: 9 | Loss: 1.4889\n",
            "Epoch: 10 | Loss: 1.3077\n",
            "Epoch: 11 | Loss: 1.1621\n",
            "Epoch: 12 | Loss: 1.0449\n",
            "Epoch: 13 | Loss: 0.9508\n",
            "Epoch: 14 | Loss: 0.8751\n",
            "Epoch: 15 | Loss: 0.8142\n",
            "Epoch: 16 | Loss: 0.7653\n",
            "Epoch: 17 | Loss: 0.7259\n",
            "Epoch: 18 | Loss: 0.6943\n",
            "Epoch: 19 | Loss: 0.6689\n",
            "Epoch: 20 | Loss: 0.6484\n",
            "Epoch: 21 | Loss: 0.6320\n",
            "Epoch: 22 | Loss: 0.6187\n",
            "Epoch: 23 | Loss: 0.6081\n",
            "Epoch: 24 | Loss: 0.5996\n",
            "Epoch: 25 | Loss: 0.5927\n",
            "Epoch: 26 | Loss: 0.5872\n",
            "Epoch: 27 | Loss: 0.5827\n",
            "Epoch: 28 | Loss: 0.5792\n",
            "Epoch: 29 | Loss: 0.5763\n",
            "Epoch: 30 | Loss: 0.5740\n",
            "Epoch: 31 | Loss: 0.5721\n",
            "Epoch: 32 | Loss: 0.5706\n",
            "Epoch: 33 | Loss: 0.5694\n",
            "Epoch: 34 | Loss: 0.5685\n",
            "Epoch: 35 | Loss: 0.5677\n",
            "Epoch: 36 | Loss: 0.5671\n",
            "Epoch: 37 | Loss: 0.5666\n",
            "Epoch: 38 | Loss: 0.5662\n",
            "Epoch: 39 | Loss: 0.5658\n",
            "Epoch: 40 | Loss: 0.5656\n",
            "Epoch: 41 | Loss: 0.5654\n",
            "Epoch: 42 | Loss: 0.5652\n",
            "Epoch: 43 | Loss: 0.5651\n",
            "Epoch: 44 | Loss: 0.5650\n",
            "Epoch: 45 | Loss: 0.5649\n",
            "Epoch: 46 | Loss: 0.5648\n",
            "Epoch: 47 | Loss: 0.5647\n",
            "Epoch: 48 | Loss: 0.5647\n",
            "Epoch: 49 | Loss: 0.5647\n",
            "Epoch: 50 | Loss: 0.5646\n",
            "Epoch: 51 | Loss: 0.5646\n",
            "Epoch: 52 | Loss: 0.5646\n",
            "Epoch: 53 | Loss: 0.5646\n",
            "Epoch: 54 | Loss: 0.5646\n",
            "Epoch: 55 | Loss: 0.5646\n",
            "Epoch: 56 | Loss: 0.5645\n",
            "Epoch: 57 | Loss: 0.5645\n",
            "Epoch: 58 | Loss: 0.5645\n",
            "Epoch: 59 | Loss: 0.5645\n",
            "Epoch: 60 | Loss: 0.5645\n",
            "Epoch: 61 | Loss: 0.5645\n",
            "Epoch: 62 | Loss: 0.5645\n",
            "Epoch: 63 | Loss: 0.5645\n",
            "Epoch: 64 | Loss: 0.5645\n",
            "Epoch: 65 | Loss: 0.5645\n",
            "Epoch: 66 | Loss: 0.5645\n",
            "Epoch: 67 | Loss: 0.5645\n",
            "Epoch: 68 | Loss: 0.5645\n",
            "Epoch: 69 | Loss: 0.5645\n",
            "Epoch: 70 | Loss: 0.5645\n",
            "Epoch: 71 | Loss: 0.5645\n",
            "Epoch: 72 | Loss: 0.5645\n",
            "Epoch: 73 | Loss: 0.5645\n",
            "Epoch: 74 | Loss: 0.5645\n",
            "Epoch: 75 | Loss: 0.5645\n",
            "Epoch: 76 | Loss: 0.5645\n",
            "Epoch: 77 | Loss: 0.5645\n",
            "Epoch: 78 | Loss: 0.5645\n",
            "Epoch: 79 | Loss: 0.5645\n",
            "Epoch: 80 | Loss: 0.5645\n",
            "Epoch: 81 | Loss: 0.5645\n",
            "Epoch: 82 | Loss: 0.5645\n",
            "Epoch: 83 | Loss: 0.5645\n",
            "Epoch: 84 | Loss: 0.5645\n",
            "Epoch: 85 | Loss: 0.5645\n",
            "Epoch: 86 | Loss: 0.5645\n",
            "Epoch: 87 | Loss: 0.5645\n",
            "Epoch: 88 | Loss: 0.5645\n",
            "Epoch: 89 | Loss: 0.5645\n",
            "Epoch: 90 | Loss: 0.5645\n",
            "Epoch: 91 | Loss: 0.5645\n",
            "Epoch: 92 | Loss: 0.5645\n",
            "Epoch: 93 | Loss: 0.5645\n",
            "Epoch: 94 | Loss: 0.5645\n",
            "Epoch: 95 | Loss: 0.5645\n",
            "Epoch: 96 | Loss: 0.5645\n",
            "Epoch: 97 | Loss: 0.5645\n",
            "Epoch: 98 | Loss: 0.5645\n",
            "Epoch: 99 | Loss: 0.5645\n",
            "Epoch: 100 | Loss: 0.5645\n",
            "Epoch: 101 | Loss: 0.5645\n",
            "Epoch: 102 | Loss: 0.5645\n",
            "Epoch: 103 | Loss: 0.5645\n",
            "Epoch: 104 | Loss: 0.5645\n",
            "Epoch: 105 | Loss: 0.5645\n",
            "Epoch: 106 | Loss: 0.5645\n",
            "Epoch: 107 | Loss: 0.5645\n",
            "Epoch: 108 | Loss: 0.5645\n",
            "Epoch: 109 | Loss: 0.5645\n",
            "Epoch: 110 | Loss: 0.5645\n",
            "Epoch: 111 | Loss: 0.5645\n",
            "Epoch: 112 | Loss: 0.5645\n",
            "Epoch: 113 | Loss: 0.5645\n",
            "Epoch: 114 | Loss: 0.5645\n",
            "Epoch: 115 | Loss: 0.5645\n",
            "Epoch: 116 | Loss: 0.5645\n",
            "Epoch: 117 | Loss: 0.5645\n",
            "Epoch: 118 | Loss: 0.5645\n",
            "Epoch: 119 | Loss: 0.5645\n",
            "Epoch: 120 | Loss: 0.5645\n",
            "Epoch: 121 | Loss: 0.5645\n",
            "Epoch: 122 | Loss: 0.5645\n",
            "Epoch: 123 | Loss: 0.5645\n",
            "Epoch: 124 | Loss: 0.5645\n",
            "Epoch: 125 | Loss: 0.5645\n",
            "Epoch: 126 | Loss: 0.5645\n",
            "Epoch: 127 | Loss: 0.5645\n",
            "Epoch: 128 | Loss: 0.5645\n",
            "Epoch: 129 | Loss: 0.5645\n",
            "Epoch: 130 | Loss: 0.5645\n",
            "Epoch: 131 | Loss: 0.5645\n",
            "Epoch: 132 | Loss: 0.5645\n",
            "Epoch: 133 | Loss: 0.5645\n",
            "Epoch: 134 | Loss: 0.5645\n",
            "Epoch: 135 | Loss: 0.5645\n",
            "Epoch: 136 | Loss: 0.5645\n",
            "Epoch: 137 | Loss: 0.5645\n",
            "Epoch: 138 | Loss: 0.5645\n",
            "Epoch: 139 | Loss: 0.5645\n",
            "Epoch: 140 | Loss: 0.5645\n",
            "Epoch: 141 | Loss: 0.5645\n",
            "Epoch: 142 | Loss: 0.5645\n",
            "Epoch: 143 | Loss: 0.5645\n",
            "Epoch: 144 | Loss: 0.5645\n",
            "Epoch: 145 | Loss: 0.5645\n",
            "Epoch: 146 | Loss: 0.5645\n",
            "Epoch: 147 | Loss: 0.5645\n",
            "Epoch: 148 | Loss: 0.5645\n",
            "Epoch: 149 | Loss: 0.5645\n"
          ]
        }
      ],
      "source": [
        "## training\n",
        "for ITER in range(150):\n",
        "    model = model.train()\n",
        "\n",
        "    ## forward\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## backward + update model params\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    print('Epoch: %d | Loss: %.4f' %(ITER, loss.detach().item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pz5BVglWnWX",
        "outputId": "cd8cf00f-78be-4bf7-a372-8624e6721442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17.096769332885742\n"
          ]
        }
      ],
      "source": [
        "sample = torch.tensor([10.0], dtype=torch.float)\n",
        "predicted = model(sample)\n",
        "print(predicted.detach().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cK1XeYVZWt_I"
      },
      "outputs": [],
      "source": [
        "model_path = 'pytorchtest.pth'\n",
        "torch.save(model, model_path, _use_new_zipfile_serialization=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
