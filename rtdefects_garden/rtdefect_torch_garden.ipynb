{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new conda env from environment.yml and run notebook in it\n",
    "!conda env create -f ./environment.yml\n",
    "!conda activate rtdefects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e181c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                     Version\n",
      "--------------------------- -----------\n",
      "absl-py                     1.4.0\n",
      "aiofiles                    22.1.0\n",
      "aiohttp                     3.8.3\n",
      "aiosignal                   1.2.0\n",
      "aiosqlite                   0.18.0\n",
      "albumentations              1.3.1\n",
      "alembic                     1.11.2\n",
      "anyio                       3.5.0\n",
      "appdirs                     1.4.4\n",
      "appnope                     0.1.2\n",
      "argon2-cffi                 21.3.0\n",
      "argon2-cffi-bindings        21.2.0\n",
      "asciitree                   0.3.3\n",
      "asttokens                   2.0.5\n",
      "astunparse                  1.6.3\n",
      "async-timeout               4.0.2\n",
      "attrs                       22.1.0\n",
      "Babel                       2.11.0\n",
      "backcall                    0.2.0\n",
      "bcrypt                      4.0.1\n",
      "beartype                    0.12.0\n",
      "beautifulsoup4              4.12.2\n",
      "bleach                      4.1.0\n",
      "blinker                     1.6.2\n",
      "bokeh                       3.2.1\n",
      "boto3                       1.28.22\n",
      "botocore                    1.31.22\n",
      "brotlipy                    0.7.0\n",
      "build                       0.10.0\n",
      "CacheControl                0.12.14\n",
      "cachetools                  4.2.2\n",
      "certifi                     2023.5.7\n",
      "cffi                        1.15.1\n",
      "chardet                     5.2.0\n",
      "charset-normalizer          2.0.4\n",
      "cleo                        2.0.1\n",
      "click                       8.1.6\n",
      "cloudpickle                 2.2.1\n",
      "colorama                    0.4.6\n",
      "comm                        0.1.2\n",
      "commonmark                  0.9.1\n",
      "contourpy                   1.1.0\n",
      "crashtest                   0.4.1\n",
      "cryptography                41.0.2\n",
      "cycler                      0.11.0\n",
      "dask                        2023.8.0\n",
      "databricks-cli              0.17.7\n",
      "debugpy                     1.6.7\n",
      "decorator                   5.1.1\n",
      "defusedxml                  0.7.1\n",
      "dill                        0.3.6\n",
      "distlib                     0.3.7\n",
      "docker                      6.1.3\n",
      "docutils                    0.20.1\n",
      "dparse                      0.6.3\n",
      "dulwich                     0.21.5\n",
      "efficientnet-pytorch        0.6.3\n",
      "entrypoints                 0.4\n",
      "exceptiongroup              1.0.4\n",
      "executing                   0.8.3\n",
      "fasteners                   0.18\n",
      "fastjsonschema              2.16.2\n",
      "filelock                    3.12.2\n",
      "flake8                      6.0.0\n",
      "Flask                       2.3.2\n",
      "Flask-Cors                  4.0.0\n",
      "flatbuffers                 2.0\n",
      "fonttools                   4.42.0\n",
      "frozenlist                  1.3.3\n",
      "fsspec                      2023.6.0\n",
      "funcx                       2.2.4\n",
      "funcx-endpoint              2.2.4\n",
      "garden-ai                   0.0.0\n",
      "gast                        0.4.0\n",
      "gitdb                       4.0.10\n",
      "GitPython                   3.1.32\n",
      "globus-compute-common       0.2.0\n",
      "globus-compute-endpoint     2.2.4\n",
      "globus-compute-sdk          2.2.4\n",
      "globus-sdk                  3.26.0\n",
      "google-auth                 2.6.0\n",
      "google-auth-oauthlib        0.5.2\n",
      "google-pasta                0.2.0\n",
      "greenlet                    2.0.1\n",
      "grpcio                      1.48.2\n",
      "gunicorn                    20.1.0\n",
      "h5py                        3.9.0\n",
      "html5lib                    1.1\n",
      "hyperspy                    1.7.5\n",
      "idna                        3.4\n",
      "imagecodecs                 2023.7.10\n",
      "imageio                     2.27.0\n",
      "importlib-metadata          6.8.0\n",
      "iniconfig                   1.1.1\n",
      "installer                   0.7.0\n",
      "ipykernel                   6.25.0\n",
      "ipyparallel                 8.6.1\n",
      "ipython                     8.12.2\n",
      "ipython-genutils            0.2.0\n",
      "itsdangerous                2.1.2\n",
      "jaraco.classes              3.3.0\n",
      "jax                         0.4.14\n",
      "jedi                        0.18.1\n",
      "Jinja2                      3.1.2\n",
      "jmespath                    1.0.1\n",
      "joblib                      1.3.2\n",
      "json5                       0.9.6\n",
      "jsonschema                  4.17.3\n",
      "jupyter_client              7.4.9\n",
      "jupyter_core                5.3.0\n",
      "jupyter-events              0.6.3\n",
      "jupyter-server              1.23.4\n",
      "jupyter_server_fileid       0.9.0\n",
      "jupyter_server_ydoc         0.8.0\n",
      "jupyter-ydoc                0.2.4\n",
      "jupyterlab                  3.6.3\n",
      "jupyterlab-pygments         0.1.2\n",
      "jupyterlab_server           2.22.0\n",
      "keras                       2.12.0\n",
      "Keras-Preprocessing         1.1.2\n",
      "keyring                     23.13.1\n",
      "kiwisolver                  1.4.4\n",
      "lazy_loader                 0.3\n",
      "libclang                    16.0.6\n",
      "llvmlite                    0.39.1\n",
      "locket                      1.0.0\n",
      "lockfile                    0.12.2\n",
      "Mako                        1.2.4\n",
      "Markdown                    3.4.1\n",
      "MarkupSafe                  2.1.1\n",
      "matplotlib                  3.7.2\n",
      "matplotlib-inline           0.1.6\n",
      "mccabe                      0.7.0\n",
      "mistune                     0.8.4\n",
      "mkl-fft                     1.3.5\n",
      "mkl-random                  1.2.2\n",
      "mkl-service                 2.4.0\n",
      "ml-dtypes                   0.2.0\n",
      "mlflow                      2.5.0\n",
      "more-itertools              10.1.0\n",
      "mpmath                      1.3.0\n",
      "msgpack                     1.0.5\n",
      "multidict                   6.0.2\n",
      "munch                       4.0.0\n",
      "natsort                     8.4.0\n",
      "nbclassic                   0.5.5\n",
      "nbclient                    0.5.13\n",
      "nbconvert                   6.4.4\n",
      "nbformat                    5.7.0\n",
      "nest-asyncio                1.5.6\n",
      "networkx                    3.1\n",
      "notebook                    6.5.4\n",
      "notebook_shim               0.2.2\n",
      "numba                       0.56.4\n",
      "numcodecs                   0.11.0\n",
      "numexpr                     2.8.5\n",
      "numpy                       1.23.5\n",
      "oauthlib                    3.2.2\n",
      "opencv-python-headless      4.8.0.76\n",
      "opt-einsum                  3.3.0\n",
      "packaging                   23.0\n",
      "pandas                      2.0.3\n",
      "pandocfilters               1.5.0\n",
      "paramiko                    3.3.1\n",
      "parsl                       2023.7.3\n",
      "parso                       0.8.3\n",
      "partd                       1.4.0\n",
      "pexpect                     4.8.0\n",
      "pickleshare                 0.7.5\n",
      "pika                        1.3.2\n",
      "Pillow                      10.0.0\n",
      "Pint                        0.22\n",
      "pip                         23.2.1\n",
      "pkginfo                     1.9.6\n",
      "platformdirs                3.10.0\n",
      "pluggy                      1.0.0\n",
      "poetry                      1.5.1\n",
      "poetry-core                 1.6.1\n",
      "poetry-plugin-export        1.4.0\n",
      "pooch                       1.4.0\n",
      "pretrainedmodels            0.7.4\n",
      "prettytable                 3.8.0\n",
      "prometheus-client           0.14.1\n",
      "prompt-toolkit              3.0.36\n",
      "protobuf                    3.20.3\n",
      "psutil                      5.9.0\n",
      "ptyprocess                  0.7.0\n",
      "pure-eval                   0.2.2\n",
      "pyarrow                     12.0.1\n",
      "pyasn1                      0.4.8\n",
      "pyasn1-modules              0.2.8\n",
      "pycodestyle                 2.10.0\n",
      "pycparser                   2.21\n",
      "pydantic                    1.10.12\n",
      "pyflakes                    3.0.1\n",
      "Pygments                    2.15.1\n",
      "PyJWT                       2.4.0\n",
      "PyNaCl                      1.5.0\n",
      "pyOpenSSL                   23.2.0\n",
      "pyparsing                   3.0.9\n",
      "pyproject_hooks             1.0.0\n",
      "pyrsistent                  0.18.0\n",
      "PySocks                     1.7.1\n",
      "pytest                      7.4.0\n",
      "python-daemon               2.3.2\n",
      "python-dateutil             2.8.2\n",
      "python-json-logger          2.0.7\n",
      "pytz                        2022.7\n",
      "PyWavelets                  1.4.1\n",
      "PyYAML                      6.0\n",
      "pyzmq                       23.2.0\n",
      "qudida                      0.0.4\n",
      "querystring-parser          1.2.4\n",
      "rapidfuzz                   2.15.1\n",
      "ratelimit                   2.2.1\n",
      "requests                    2.31.0\n",
      "requests-oauthlib           1.3.0\n",
      "requests-toolbelt           1.0.0\n",
      "rfc3339-validator           0.1.4\n",
      "rfc3986-validator           0.1.1\n",
      "rich                        12.6.0\n",
      "rsa                         4.7.2\n",
      "s3transfer                  0.6.1\n",
      "scikit-image                0.21.0\n",
      "scikit-learn                1.3.0\n",
      "scipy                       1.11.1\n",
      "segmentation-models-pytorch 0.2.1\n",
      "Send2Trash                  1.8.0\n",
      "setproctitle                1.3.2\n",
      "setuptools                  68.0.0\n",
      "shellingham                 1.5.0.post1\n",
      "six                         1.16.0\n",
      "smmap                       5.0.0\n",
      "sniffio                     1.2.0\n",
      "soupsieve                   2.4\n",
      "sparse                      0.14.0\n",
      "SQLAlchemy                  2.0.19\n",
      "sqlparse                    0.4.4\n",
      "stack-data                  0.2.0\n",
      "sympy                       1.12\n",
      "tabulate                    0.9.0\n",
      "tblib                       1.7.0\n",
      "tensorboard                 2.12.1\n",
      "tensorboard-data-server     0.7.0\n",
      "tensorboard-plugin-wit      1.6.0\n",
      "tensorflow                  2.12.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-estimator        2.12.0\r\n",
      "termcolor                   2.1.0\r\n",
      "terminado                   0.17.1\r\n",
      "testpath                    0.6.0\r\n",
      "texttable                   1.6.7\r\n",
      "threadpoolctl               3.2.0\r\n",
      "tifffile                    2023.7.18\r\n",
      "timm                        0.4.12\r\n",
      "tomli                       2.0.1\r\n",
      "tomlkit                     0.12.1\r\n",
      "toolz                       0.12.0\r\n",
      "torch                       2.0.1\r\n",
      "torchvision                 0.15.2\r\n",
      "tornado                     6.3.2\r\n",
      "tqdm                        4.66.0\r\n",
      "trackpy                     0.5.0\r\n",
      "traitlets                   5.7.1\r\n",
      "traits                      6.4.2\r\n",
      "trove-classifiers           2023.7.6\r\n",
      "typeguard                   2.13.3\r\n",
      "typer                       0.7.0\r\n",
      "types-paramiko              3.3.0.0\r\n",
      "types-requests              2.31.0.2\r\n",
      "types-six                   1.16.21.9\r\n",
      "types-tabulate              0.9.0.3\r\n",
      "types-urllib3               1.26.25.14\r\n",
      "typing_extensions           4.7.1\r\n",
      "tzdata                      2023.3\r\n",
      "urllib3                     1.26.16\r\n",
      "virtualenv                  20.24.2\r\n",
      "watchdog                    3.0.0\r\n",
      "wcwidth                     0.2.5\r\n",
      "webencodings                0.5.1\r\n",
      "websocket-client            0.58.0\r\n",
      "websockets                  10.3\r\n",
      "Werkzeug                    2.3.6\r\n",
      "wheel                       0.35.1\r\n",
      "wrapt                       1.14.1\r\n",
      "xattr                       0.10.1\r\n",
      "xyzservices                 2023.7.0\r\n",
      "y-py                        0.5.9\r\n",
      "yarl                        1.8.1\r\n",
      "ypy-websocket               0.8.2\r\n",
      "zarr                        2.16.0\r\n",
      "zipp                        3.16.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e052d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python must be version 3.10.*\n",
    "import sys\n",
    "assert sys.version_info[0] == 3 and sys.version_info[1] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24850e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import garden_ai\n",
    "from garden_ai import step, GardenClient\n",
    "\n",
    "import json\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from hashlib import md5\n",
    "from skimage import color, measure, morphology\n",
    "from io import BytesIO\n",
    "from time import perf_counter\n",
    "from hyperspy import io as hsio\n",
    "from scipy.stats import siegelslopes\n",
    "from scipy.interpolate import interp1d\n",
    "import albumentations as albu\n",
    "import imageio.v2 as imageio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcfc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = GardenClient()\n",
    "\n",
    "# First, we needed to register our pretrained ML model with the garden service.\n",
    "# When we registered the pretrained model (model.pth copied in this directory) via `$ garden-ai model register ...`\n",
    "# We were given this model name, which we can use to reference the model in a pipeline.\n",
    "# All rtdefect models included in ./models directory\n",
    "REGISTERED_MODEL_NAME_1 = \"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\" #small_voids_031023.pth\n",
    "REGISTERED_MODEL_NAME_2 = \"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\" #voids_segmentation_091321.pth\n",
    "REGISTERED_MODEL_NAME_3 = \"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\" #voids_segmentation_030323.pth\n",
    "\n",
    "TEST_INPUT_PATH = \"./data/input_image.tiff\"\n",
    "TEST_OUTPUT_MASK_PATH_1 = \"./data/torch_1_output_mask.tiff\"\n",
    "TEST_OUTPUT_MASK_PATH_2 = \"./data/torch_2_output_mask.tiff\"\n",
    "TEST_OUTPUT_MASK_PATH_3 = \"./data/torch_3_output_mask.tiff\"\n",
    "TEST_OUTPUT_DEFECT_PATH_1 = \"./data/torch_1_output_defect_results.json\"\n",
    "TEST_OUTPUT_DEFECT_PATH_2 = \"./data/torch_2_output_defect_results.json\"\n",
    "TEST_OUTPUT_DEFECT_PATH_3 = \"./data/torch_3_output_defect_results.json\"\n",
    "PIPELINE_DOI_1 = \"10.23677/b246-hj14\"\n",
    "PIPELINE_DOI_2 = \"10.23677/xn48-pr25\"\n",
    "PIPELINE_DOI_3 = \"10.23677/5jzj-0j60\"\n",
    "GARDEN_DOI = \"10.23677/nzhf-rq49\"\n",
    "PIP_REQUIREMENTS = [\"torchvision==0.15.2\", \"torch==2.0.1\", \"segmentation_models.pytorch==0.2.*\", \"pandas==2.0.3\", \"scikit-image==0.21.0\", \"chardet==5.2.0\", \"hyperspy==1.7.5\", \"werkzeug==2.2.3\", \"albumentations==1.3.1\"]\n",
    "CONDA_REQUIREMENTS = [\"tensorflow>2\", \"nomkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcacfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a our pipelines steps\n",
    "# Decorate it with `@step` so that we can use it to build up a pipeline\n",
    "\n",
    "@step\n",
    "def preprocessing_all(\n",
    "    input_data: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    from typing import Optional, Tuple\n",
    "    from io import BytesIO\n",
    "    from skimage import color, measure, morphology\n",
    "    from skimage.transform import resize\n",
    "    import numpy as np\n",
    "    import imageio.v2 as imageio\n",
    "\n",
    "    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\n",
    "        # Convert mask to a uint8-compatible image\n",
    "        data = np.squeeze(data)\n",
    "        assert data.ndim == 2, \"Image must be grayscale\"\n",
    "        assert np.logical_and(data >= 0, data <= 1).all(), \"Image values must be between 0 and 1\"\n",
    "        data = np.array(data * 255, dtype=np.uint8)\n",
    "\n",
    "        # Convert mask to a TIFF-encoded image\n",
    "        output_img = BytesIO()\n",
    "        writer = imageio.get_writer(output_img, format='tiff', mode='i')\n",
    "        writer.append_data(data, meta={'compression': compress_type})\n",
    "        return output_img.getvalue()\n",
    "    \n",
    "    #Encode image data as tiff\n",
    "    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\n",
    "\n",
    "    # Load the TIFF file into a numpy array\n",
    "    image_gray = imageio.imread(BytesIO(encoded_image_data))\n",
    "\n",
    "    # Convert to RGB\n",
    "    image: np.ndarray = color.gray2rgb(image_gray)\n",
    "\n",
    "    # Scale to 1024x1024\n",
    "    if image.shape[:2] != (1024, 1024):\n",
    "        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\n",
    "\n",
    "    return image\n",
    "\n",
    "@step\n",
    "def preprocessing_torch_1(\n",
    "    input_data: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import torch\n",
    "    \n",
    "    MODEL_NAME = \"small_voids_031023.pth\"\n",
    "    \n",
    "    # Define the conversion from image to inputs\n",
    "    def to_tensor(x: np.ndarray, **kwargs):\n",
    "        return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "    _encoders = {\n",
    "        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\n",
    "        'voids_segmentation_030323.pth': 'efficientnet-b3',\n",
    "        'small_voids_031023.pth': 'se_resnext50_32x4d',\n",
    "    }\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    preprocess = albu.Compose(_transform)\n",
    "\n",
    "    # Perform the preprocessing\n",
    "    image = preprocess(image=input_data)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\n",
    "\n",
    "@step\n",
    "def preprocessing_torch_2(\n",
    "    input_data: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import torch\n",
    "    \n",
    "    MODEL_NAME = \"voids_segmentation_091321.pth\"\n",
    "    \n",
    "    # Define the conversion from image to inputs\n",
    "    def to_tensor(x: np.ndarray, **kwargs):\n",
    "        return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "    _encoders = {\n",
    "        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\n",
    "        'voids_segmentation_030323.pth': 'efficientnet-b3',\n",
    "        'small_voids_031023.pth': 'se_resnext50_32x4d',\n",
    "    }\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    preprocess = albu.Compose(_transform)\n",
    "\n",
    "    # Perform the preprocessing\n",
    "    image = preprocess(image=input_data)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\n",
    "\n",
    "@step\n",
    "def preprocessing_torch_3(\n",
    "    input_data: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import torch\n",
    "    \n",
    "    MODEL_NAME = \"voids_segmentation_030323.pth\"\n",
    "    \n",
    "    # Define the conversion from image to inputs\n",
    "    def to_tensor(x: np.ndarray, **kwargs):\n",
    "        return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "    _encoders = {\n",
    "        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\n",
    "        'voids_segmentation_030323.pth': 'efficientnet-b3',\n",
    "        'small_voids_031023.pth': 'se_resnext50_32x4d',\n",
    "    }\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    preprocess = albu.Compose(_transform)\n",
    "\n",
    "    # Perform the preprocessing\n",
    "    image = preprocess(image=input_data)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\n",
    "\n",
    "@step\n",
    "def run_inference_model_1(\n",
    "    input_data: torch.Tensor,\n",
    "    model=garden_ai.Model(REGISTERED_MODEL_NAME_1),  # loads the registered model by name, with a `.predict()` method\n",
    ") -> torch.Tensor:\n",
    "    return model.predict(input_data)\n",
    "\n",
    "@step\n",
    "def run_inference_model_2(\n",
    "    input_data: torch.Tensor,\n",
    "    model=garden_ai.Model(REGISTERED_MODEL_NAME_2),  # loads the registered model by name, with a `.predict()` method\n",
    ") -> torch.Tensor:\n",
    "    return model.predict(input_data)\n",
    "\n",
    "@step\n",
    "def run_inference_model_3(\n",
    "    input_data: torch.Tensor,\n",
    "    model=garden_ai.Model(REGISTERED_MODEL_NAME_3),  # loads the registered model by name, with a `.predict()` method\n",
    ") -> torch.Tensor:\n",
    "    return model.predict(input_data)\n",
    "    \n",
    "@step\n",
    "def postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    from io import BytesIO\n",
    "    from skimage import color, measure, morphology\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import imageio.v2 as imageio\n",
    "    import torch\n",
    "\n",
    "    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\n",
    "        # Convert mask to a uint8-compatible image\n",
    "        data = np.squeeze(data)\n",
    "        assert data.ndim == 2, \"Image must be grayscale\"\n",
    "        assert np.logical_and(data >= 0, data <= 1).all(), \"Image values must be between 0 and 1\"\n",
    "        data = np.array(data * 255, dtype=np.uint8)\n",
    "\n",
    "        # Convert mask to a TIFF-encoded image\n",
    "        output_img = BytesIO()\n",
    "        writer = imageio.get_writer(output_img, format='tiff', mode='i')\n",
    "        writer.append_data(data, meta={'compression': compress_type})\n",
    "        return output_img.getvalue()\n",
    "\n",
    "    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\n",
    "        mask = morphology.remove_small_objects(mask, min_size=min_size)\n",
    "        mask = morphology.remove_small_holes(mask, min_size)\n",
    "        mask = morphology.binary_erosion(mask, morphology.square(1))\n",
    "        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\n",
    "\n",
    "        # Assign labels to the labeled regions\n",
    "        labels = measure.label(mask)\n",
    "        output['void_count'] = int(labels.max())\n",
    "\n",
    "        # Compute region properties\n",
    "        props = measure.regionprops(labels, mask)\n",
    "        radii = [p['equivalent_diameter'] / 2 for p in props]\n",
    "        output['radii'] = radii\n",
    "        output['radii_average'] = np.average(radii)\n",
    "        output['positions'] = [p['centroid'] for p in props]\n",
    "        return output, labels\n",
    "    \n",
    "    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # Make it into a bool array\n",
    "    segment = np.squeeze(input_data_numpy)\n",
    "    mask = segment > 0.9\n",
    "\n",
    "    # Generate the analysis results\n",
    "    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\n",
    "\n",
    "    # Convert mask to a TIFF-encoded image\n",
    "    mask_data = encode_as_tiff(mask)\n",
    "    \n",
    "    output = {\"mask\" : mask_data, \"defect_results\" : defect_results}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889e304e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created pipelines\n"
     ]
    }
   ],
   "source": [
    "# make a simple Pipeline using the steps we just defined\n",
    "rtdefect_pipeline_1 = client.create_pipeline(\n",
    "    title=\"RT Defect Analysis Torch 1 Demo Pipeline\",\n",
    "    python_version=f\"{sys.version_info[0]}.{sys.version_info[1]}.{sys.version_info[2]}\",\n",
    "    pip_dependencies=PIP_REQUIREMENTS,\n",
    "    conda_dependencies=CONDA_REQUIREMENTS,\n",
    "    steps=(preprocessing_all, preprocessing_torch_1, run_inference_model_1, postprocessing_all),  # steps run in order, passing output to subsequent steps\n",
    "    authors=[\n",
    "        \"Ward, Logan\",\n",
    "    ],\n",
    "    contributors=[\"Tuecke, Max\"],\n",
    "    version=\"0.0.1\",\n",
    "    year=2023,\n",
    "    tags=[],\n",
    "    short_name=\"rtdefect_torch_1\",\n",
    "    doi=PIPELINE_DOI_1,\n",
    ")\n",
    "\n",
    "\n",
    "rtdefect_pipeline_2 = client.create_pipeline(\n",
    "    title=\"RT Defect Analysis Torch 2 Demo Pipeline\",\n",
    "    python_version=f\"{sys.version_info[0]}.{sys.version_info[1]}.{sys.version_info[2]}\",\n",
    "    pip_dependencies=PIP_REQUIREMENTS,\n",
    "    conda_dependencies=CONDA_REQUIREMENTS,\n",
    "    steps=(preprocessing_all, preprocessing_torch_2, run_inference_model_2, postprocessing_all),  # steps run in order, passing output to subsequent steps\n",
    "    authors=[\n",
    "        \"Ward, Logan\",\n",
    "    ],\n",
    "    contributors=[\"Tuecke, Max\"],\n",
    "    version=\"0.0.1\",\n",
    "    year=2023,\n",
    "    tags=[],\n",
    "    short_name=\"rtdefect_torch_2\",\n",
    "    doi=PIPELINE_DOI_2,\n",
    ")\n",
    "\n",
    "rtdefect_pipeline_3 = client.create_pipeline(\n",
    "    title=\"RT Defect Analysis Torch 3 Demo Pipeline\",\n",
    "    python_version=f\"{sys.version_info[0]}.{sys.version_info[1]}.{sys.version_info[2]}\",\n",
    "    pip_dependencies=PIP_REQUIREMENTS,\n",
    "    conda_dependencies=CONDA_REQUIREMENTS,\n",
    "    steps=(preprocessing_all, preprocessing_torch_3, run_inference_model_3, postprocessing_all),  # steps run in order, passing output to subsequent steps\n",
    "    authors=[\n",
    "        \"Ward, Logan\",\n",
    "    ],\n",
    "    contributors=[\"Tuecke, Max\"],\n",
    "    version=\"0.0.1\",\n",
    "    year=2023,\n",
    "    tags=[],\n",
    "    short_name=\"rtdefect_torch_3\",\n",
    "    doi=PIPELINE_DOI_3,\n",
    ")\n",
    "\n",
    "print(\"Created pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4925c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container ID: d81c62ed-9df6-4ecf-a5a3-4e7e666c2917\n",
      "Registered pipeline '10.23677/b246-hj14'!\n",
      "Registered pipeline '10.23677/xn48-pr25'!\n",
      "Registered pipeline '10.23677/5jzj-0j60'!\n"
     ]
    }
   ],
   "source": [
    "# now we need to register the pipeline for remote execution\n",
    "# build a container with the pipeline's specific dependencies/python version\n",
    "# then register the pipeline so that it will execute from that container\n",
    "container_id = \"d81c62ed-9df6-4ecf-a5a3-4e7e666c2917\" # (reuse a known container uuid to skip slow build step)\n",
    "#container_id = client.build_container(rtdefect_pipeline_1) # <-- to build a fresh container\n",
    "print(f\"Container ID: {container_id}\")\n",
    "\n",
    "client.register_pipeline(rtdefect_pipeline_1, container_id)\n",
    "print(f\"Registered pipeline '{rtdefect_pipeline_1.doi}'!\")\n",
    "\n",
    "client.register_pipeline(rtdefect_pipeline_2, container_id)\n",
    "print(f\"Registered pipeline '{rtdefect_pipeline_2.doi}'!\")\n",
    "\n",
    "client.register_pipeline(rtdefect_pipeline_3, container_id)\n",
    "print(f\"Registered pipeline '{rtdefect_pipeline_3.doi}'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717bc553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:hyperspy.io:If this file format is supported, please report this error to the HyperSpy developers.\n"
     ]
    }
   ],
   "source": [
    "# now that we've registered our pipeline, we can test its remote execution against some sample input\n",
    "def load_rtdefects_input(path: Path) -> np.ndarray:\n",
    "    # Step 1: attempt to read it with imageio\n",
    "    load_functions = [\n",
    "        imageio.imread,\n",
    "        lambda x: hsio.load(x).data\n",
    "    ]\n",
    "    data = None\n",
    "    for function in load_functions:\n",
    "        try:\n",
    "            data: np.ndarray = function(path)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    if data is None:\n",
    "        raise ValueError(f'Failed to load image from {path}')\n",
    "\n",
    "    # Standardize the format\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    data = np.squeeze(data)\n",
    "    if data.ndim == 3:\n",
    "        data = color.rgb2gray(data)\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    return data\n",
    "\n",
    "demo_input = load_rtdefects_input(TEST_INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4760e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline 1 remote execution.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline 2 remote execution.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline 3 remote execution.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! All pipelines executed with correct results.\n"
     ]
    }
   ],
   "source": [
    "# results we want to reproduce:\n",
    "with open(TEST_OUTPUT_MASK_PATH_1, \"rb\") as img:\n",
    "\texpected_mask_1 = img.read()\n",
    "expected_defects_1 = json.load(open(TEST_OUTPUT_DEFECT_PATH_1))\n",
    "\n",
    "with open(TEST_OUTPUT_MASK_PATH_2, \"rb\") as img:\n",
    "\texpected_mask_2 = img.read()\n",
    "expected_defects_2 = json.load(open(TEST_OUTPUT_DEFECT_PATH_2))\n",
    "\n",
    "with open(TEST_OUTPUT_MASK_PATH_3, \"rb\") as img:\n",
    "\texpected_mask_3 = img.read()\n",
    "expected_defects_3 = json.load(open(TEST_OUTPUT_DEFECT_PATH_3))\n",
    "\n",
    "\n",
    "# to run remotely, use the client to fetch our newly registered pipeline --\n",
    "# note that our pipeline is only available to us at this point;\n",
    "# it can't be discovered/used by others until it's part of a published Garden\n",
    "print(\"Starting pipeline 1 remote execution.\")\n",
    "rtdefect_remote_1 = client.get_registered_pipeline(PIPELINE_DOI_1)\n",
    "results_1 = rtdefect_remote_1(\n",
    "    demo_input,\n",
    "    endpoint=\"6d39d01e-2955-47b9-a1f6-50f147e650d6\",  # execute on Globus Compute endpoint of choice\n",
    ")\n",
    "assert results_1[\"mask\"] == expected_mask_1\n",
    "assert json.loads(json.dumps(results_1[\"defect_results\"])) == expected_defects_1\n",
    "\n",
    "\n",
    "print(\"Starting pipeline 2 remote execution.\")\n",
    "rtdefect_remote_2 = client.get_registered_pipeline(PIPELINE_DOI_2)\n",
    "results_2 = rtdefect_remote_2(\n",
    "    demo_input,\n",
    "    endpoint=\"6d39d01e-2955-47b9-a1f6-50f147e650d6\",  # execute on Globus Compute endpoint of choice\n",
    ")\n",
    "assert results_2[\"mask\"] == expected_mask_2\n",
    "assert json.loads(json.dumps(results_2[\"defect_results\"])) == expected_defects_2\n",
    "\n",
    "\n",
    "print(\"Starting pipeline 3 remote execution.\")\n",
    "rtdefect_remote_3 = client.get_registered_pipeline(PIPELINE_DOI_3)\n",
    "results_3 = rtdefect_remote_3(\n",
    "    demo_input,\n",
    "    endpoint=\"6d39d01e-2955-47b9-a1f6-50f147e650d6\",  # execute on Globus Compute endpoint of choice\n",
    ")\n",
    "assert results_3[\"mask\"] == expected_mask_3\n",
    "assert json.loads(json.dumps(results_3[\"defect_results\"])) == expected_defects_3\n",
    "\n",
    "\n",
    "print(\"Done! All pipelines executed with correct results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a0b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've sanity-checked the pipeline's remote execution, we can publish it as part of a Garden:\n",
    "rtdefect_garden_torch = client.create_garden(\n",
    "    title=\"RT Defect Analysis Torch Demo Garden\",\n",
    "    authors=[\"Max Tuecke\"],\n",
    "    description=\"Recreates the RT Defect Analysis pytorch model from https://github.com/ivem-argonne/real-time-defect-analysis/tree/main\",\n",
    "    doi=GARDEN_DOI,\n",
    ")\n",
    "# include the pipeline by just its DOI:\n",
    "rtdefect_garden_torch.pipeline_ids += [PIPELINE_DOI_1]\n",
    "rtdefect_garden_torch.pipeline_ids += [PIPELINE_DOI_2]\n",
    "rtdefect_garden_torch.pipeline_ids += [PIPELINE_DOI_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d42085de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, publish our new garden, making it (and its pipeline) discoverable by other garden users\n",
    "# (see example discovery/execution flow of this garden and pipeline in rtdefect_garden_remote_inference.ipynb)\n",
    "client.publish_garden_metadata(rtdefect_garden_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fef59fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m{\u001b[0m\r\n",
      "  \u001b[1;34m\"gmeta\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "    \u001b[1m{\u001b[0m\r\n",
      "      \u001b[1;34m\"@datatype\"\u001b[0m: \u001b[32m\"GMetaResult\"\u001b[0m,\r\n",
      "      \u001b[1;34m\"entries\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "        \u001b[1m{\u001b[0m\r\n",
      "          \u001b[1;34m\"content\"\u001b[0m: \u001b[1m{\u001b[0m\r\n",
      "            \u001b[1;34m\"pipeline_aliases\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\r\n",
      "            \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"description\"\u001b[0m: \u001b[32m\"Recreates the RT Defect Analysis pytorch model from https://github.com/ivem-argonne/real-time-defect-analysis/tree/main\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"language\"\u001b[0m: \u001b[32m\"en\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch Demo Garden\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"pipelines\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[1m{\u001b[0m\r\n",
      "                \u001b[1;34m\"models\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"flavor\"\u001b[0m: \u001b[32m\"pytorch\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"user_email\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"full_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_name\"\u001b[0m: \u001b[32m\"rtdefect-torch-model-1-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"serialize_type\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"mlflow_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com-rtdefect-torch-model-1-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"dataset\"\u001b[0m: \u001b[3;35mnull\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                \u001b[1;34m\"func_uuid\"\u001b[0m: \u001b[32m\"434a28d9-2e90-4b2d-80df-1ecfcc748f9b\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch 1 Demo Pipeline\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"steps\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_all: (input_data: numpy.ndarray) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_all(\\n    input_data: np.ndarray,\\n) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    from skimage.transform import resize\\n    import numpy as np\\n    import imageio.v2 as imageio\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n    \\n    #Encode image data as tiff\\n    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\\n\\n    # Load the TIFF file into a numpy array\\n    image_gray = imageio.imread(BytesIO(encoded_image_data))\\n\\n    # Convert to RGB\\n    image: np.ndarray = color.gray2rgb(image_gray)\\n\\n    # Scale to 1024x1024\\n    if image.shape[:2] != (1024, 1024):\\n        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\\n\\n    return image\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_torch_1: (input_data: numpy.ndarray) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_torch_1(\\n    input_data: np.ndarray,\\n) -> torch.Tensor:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import torch\\n    \\n    MODEL_NAME = \\\"small_voids_031023.pth\\\"\\n    \\n    # Define the conversion from image to inputs\\n    def to_tensor(x: np.ndarray, **kwargs):\\n        return x.transpose(2, 0, 1).astype('float32')\\n\\n    _encoders = {\\n        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\\n        'voids_segmentation_030323.pth': 'efficientnet-b3',\\n        'small_voids_031023.pth': 'se_resnext50_32x4d',\\n    }\\n    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\\n    \\n    _transform = [\\n        albu.Lambda(image=preprocessing_fn),\\n        albu.Lambda(image=to_tensor),\\n    ]\\n    preprocess = albu.Compose(_transform)\\n\\n    # Perform the preprocessing\\n    image = preprocess(image=input_data)\\n\\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_torch_1\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"run_inference_model_1: (input_data: torch.Tensor, model=<__main__._Model object at 0x7f8e6ef18340>) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                      \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\"\u001b[0m\r\n",
      "                    \u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef run_inference_model_1(\\n    input_data: torch.Tensor,\\n    model=garden_ai.Model(REGISTERED_MODEL_NAME_1),  # loads the registered model by name, with a `.predict()` method\\n) -> torch.Tensor:\\n    return model.predict(input_data)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"run_inference_model_1\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"postprocessing_all: (input_data: torch.Tensor) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import imageio.v2 as imageio\\n    import torch\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n\\n    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\\n        mask = morphology.remove_small_objects(mask, min_size=min_size)\\n        mask = morphology.remove_small_holes(mask, min_size)\\n        mask = morphology.binary_erosion(mask, morphology.square(1))\\n        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\\n\\n        # Assign labels to the labeled regions\\n        labels = measure.label(mask)\\n        output['void_count'] = int(labels.max())\\n\\n        # Compute region properties\\n        props = measure.regionprops(labels, mask)\\n        radii = [p['equivalent_diameter'] / 2 for p in props]\\n        output['radii'] = radii\\n        output['radii_average'] = np.average(radii)\\n        output['positions'] = [p['centroid'] for p in props]\\n        return output, labels\\n    \\n    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\\n\\n    # Make it into a bool array\\n    segment = np.squeeze(input_data_numpy)\\n    mask = segment > 0.9\\n\\n    # Generate the analysis results\\n    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\\n\\n    # Convert mask to a TIFF-encoded image\\n    mask_data = encode_as_tiff(mask)\\n    \\n    output = {\\\"mask\\\" : mask_data, \\\"defect_results\\\" : defect_results}\\n    \\n    return output\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"postprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"conda_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"nomkl\"\u001b[0m,\r\n",
      "                  \u001b[32m\"tensorflow>2\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"papers\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"pip_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"torchvision==0.15.2\"\u001b[0m,\r\n",
      "                  \u001b[32m\"mlflow-skinny==2.5.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"albumentations==1.3.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"scikit-image==0.21.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"chardet==5.2.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"segmentation_models.pytorch==0.2.*\"\u001b[0m,\r\n",
      "                  \u001b[32m\"werkzeug==2.2.3\"\u001b[0m,\r\n",
      "                  \u001b[32m\"torch==2.0.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"hyperspy==1.7.5\"\u001b[0m,\r\n",
      "                  \u001b[32m\"pandas==2.0.3\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"repositories\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"python_version\"\u001b[0m: \u001b[32m\"3.10.12\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"short_name\"\u001b[0m: \u001b[32m\"rtdefect_torch_1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/b246-hj14\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Ward, Logan\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m\r\n",
      "              \u001b[1m}\u001b[0m,\r\n",
      "              \u001b[1m{\u001b[0m\r\n",
      "                \u001b[1;34m\"models\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"flavor\"\u001b[0m: \u001b[32m\"pytorch\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"user_email\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"full_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_name\"\u001b[0m: \u001b[32m\"rtdefect-torch-model-2-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"serialize_type\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"mlflow_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com-rtdefect-torch-model-2-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"dataset\"\u001b[0m: \u001b[3;35mnull\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                \u001b[1;34m\"func_uuid\"\u001b[0m: \u001b[32m\"39ffb237-3e6c-498e-a798-de8e55190767\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch 2 Demo Pipeline\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"steps\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_all: (input_data: numpy.ndarray) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_all(\\n    input_data: np.ndarray,\\n) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    from skimage.transform import resize\\n    import numpy as np\\n    import imageio.v2 as imageio\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n    \\n    #Encode image data as tiff\\n    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\\n\\n    # Load the TIFF file into a numpy array\\n    image_gray = imageio.imread(BytesIO(encoded_image_data))\\n\\n    # Convert to RGB\\n    image: np.ndarray = color.gray2rgb(image_gray)\\n\\n    # Scale to 1024x1024\\n    if image.shape[:2] != (1024, 1024):\\n        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\\n\\n    return image\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r",
      "\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_torch_2: (input_data: numpy.ndarray) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_torch_2(\\n    input_data: np.ndarray,\\n) -> torch.Tensor:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import torch\\n    \\n    MODEL_NAME = \\\"voids_segmentation_091321.pth\\\"\\n    \\n    # Define the conversion from image to inputs\\n    def to_tensor(x: np.ndarray, **kwargs):\\n        return x.transpose(2, 0, 1).astype('float32')\\n\\n    _encoders = {\\n        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\\n        'voids_segmentation_030323.pth': 'efficientnet-b3',\\n        'small_voids_031023.pth': 'se_resnext50_32x4d',\\n    }\\n    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\\n    \\n    _transform = [\\n        albu.Lambda(image=preprocessing_fn),\\n        albu.Lambda(image=to_tensor),\\n    ]\\n    preprocess = albu.Compose(_transform)\\n\\n    # Perform the preprocessing\\n    image = preprocess(image=input_data)\\n\\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \\n    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_torch_2\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"run_inference_model_2: (input_data: torch.Tensor, model=<__main__._Model object at 0x7f8e80a97040>) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                      \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\"\u001b[0m\r\n",
      "                    \u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef run_inference_model_2(\\n    input_data: torch.Tensor,\\n    model=garden_ai.Model(REGISTERED_MODEL_NAME_2),  # loads the registered model by name, with a `.predict()` method\\n) -> torch.Tensor:\\n    return model.predict(input_data)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"run_inference_model_2\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"postprocessing_all: (input_data: torch.Tensor) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import imageio.v2 as imageio\\n    import torch\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n\\n    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\\n        mask = morphology.remove_small_objects(mask, min_size=min_size)\\n        mask = morphology.remove_small_holes(mask, min_size)\\n        mask = morphology.binary_erosion(mask, morphology.square(1))\\n        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\\n\\n        # Assign labels to the labeled regions\\n        labels = measure.label(mask)\\n        output['void_count'] = int(labels.max())\\n\\n        # Compute region properties\\n        props = measure.regionprops(labels, mask)\\n        radii = [p['equivalent_diameter'] / 2 for p in props]\\n        output['radii'] = radii\\n        output['radii_average'] = np.average(radii)\\n        output['positions'] = [p['centroid'] for p in props]\\n        return output, labels\\n    \\n    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\\n\\n    # Make it into a bool array\\n    segment = np.squeeze(input_data_numpy)\\n    mask = segment > 0.9\\n\\n    # Generate the analysis results\\n    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\\n\\n    # Convert mask to a TIFF-encoded image\\n    mask_data = encode_as_tiff(mask)\\n    \\n    output = {\\\"mask\\\" : mask_data, \\\"defect_results\\\" : defect_results}\\n    \\n    return output\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"postprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"conda_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"nomkl\"\u001b[0m,\r\n",
      "                  \u001b[32m\"tensorflow>2\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"papers\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"pip_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"torchvision==0.15.2\"\u001b[0m,\r\n",
      "                  \u001b[32m\"mlflow-skinny==2.5.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"albumentations==1.3.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"scikit-image==0.21.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"chardet==5.2.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"segmentation_models.pytorch==0.2.*\"\u001b[0m,\r\n",
      "                  \u001b[32m\"werkzeug==2.2.3\"\u001b[0m,\r\n",
      "                  \u001b[32m\"torch==2.0.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"hyperspy==1.7.5\"\u001b[0m,\r\n",
      "                  \u001b[32m\"pandas==2.0.3\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"repositories\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"python_version\"\u001b[0m: \u001b[32m\"3.10.12\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"short_name\"\u001b[0m: \u001b[32m\"rtdefect_torch_2\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/xn48-pr25\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Ward, Logan\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m\r\n",
      "              \u001b[1m}\u001b[0m,\r\n",
      "              \u001b[1m{\u001b[0m\r\n",
      "                \u001b[1;34m\"models\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"flavor\"\u001b[0m: \u001b[32m\"pytorch\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"user_email\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"full_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_name\"\u001b[0m: \u001b[32m\"rtdefect-torch-model-3-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"serialize_type\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"mlflow_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com-rtdefect-torch-model-3-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"dataset\"\u001b[0m: \u001b[3;35mnull\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                \u001b[1;34m\"func_uuid\"\u001b[0m: \u001b[32m\"4e82243b-1394-491a-b31c-92c279e1f4b5\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch 3 Demo Pipeline\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"steps\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_all: (input_data: numpy.ndarray) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_all(\\n    input_data: np.ndarray,\\n) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    from skimage.transform import resize\\n    import numpy as np\\n    import imageio.v2 as imageio\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n    \\n    #Encode image data as tiff\\n    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\\n\\n    # Load the TIFF file into a numpy array\\n    image_gray = imageio.imread(BytesIO(encoded_image_data))\\n\\n    # Convert to RGB\\n    image: np.ndarray = color.gray2rgb(image_gray)\\n\\n    # Scale to 1024x1024\\n    if image.shape[:2] != (1024, 1024):\\n        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\\n\\n    return image\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_torch_3: (input_data: numpy.ndarray) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_torch_3(\\n    input_data: np.ndarray,\\n) -> torch.Tensor:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import torch\\n    \\n    MODEL_NAME = \\\"voids_segmentation_030323.pth\\\"\\n    \\n    # Define the conversion from image to inputs\\n    def to_tensor(x: np.ndarray, **kwargs):\\n        return x.transpose(2, 0, 1).astype('float32')\\n\\n    _encoders = {\\n        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\\n        'voids_segmentation_030323.pth': 'efficientnet-b3',\\n        'small_voids_031023.pth': 'se_resnext50_32x4d',\\n    }\\n    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\\n    \\n    _transform = [\\n        albu.Lambda(image=preprocessing_fn),\\n        albu.Lambda(image=to_tensor),\\n    ]\\n    preprocess = albu.Compose(_transform)\\n\\n    # Perform the preprocessing\\n    image = preprocess(image=input_data)\\n\\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \\n    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_torch_3\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"run_inference_model_3: (input_data: torch.Tensor, model=<__main__._Model object at 0x7f8e80a97580>) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                      \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\"\u001b[0m\r\n",
      "                    \u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef run_inference_model_3(\\n    input_data: torch.Tensor,\\n    model=garden_ai.Model(REGISTERED_MODEL_NAME_3),  # loads the registered model by name, with a `.predict()` method\\n) -> torch.Tensor:\\n    return model.predict(input_data)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"run_inference_model_3\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"postprocessing_all: (input_data: torch.Tensor) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import imageio.v2 as imageio\\n    import torch\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n\\n    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\\n        mask = morphology.remove_small_objects(mask, min_size=min_size)\\n        mask = morphology.remove_small_holes(mask, min_size)\\n        mask = morphology.binary_erosion(mask, morphology.square(1))\\n        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\\n\\n        # Assign labels to the labeled regions\\n        labels = measure.label(mask)\\n        output['void_count'] = int(labels.max())\\n\\n        # Compute region properties\\n        props = measure.regionprops(labels, mask)\\n        radii = [p['equivalent_diameter'] / 2 for p in props]\\n        output['radii'] = radii\\n        output['radii_average'] = np.average(radii)\\n        output['positions'] = [p['centroid'] for p in props]\\n        return output, labels\\n    \\n    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\\n\\n    # Make it into a bool array\\n    segment = np.squeeze(input_data_numpy)\\n    mask = segment > 0.9\\n\\n    # Generate the analysis results\\n    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\\n\\n    # Convert mask to a TIFF-encoded image\\n    mask_data = encode_as_tiff(mask)\\n    \\n    output = {\\\"mask\\\" : mask_data, \\\"defect_results\\\" : defect_results}\\n    \\n    return output\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"postprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"conda_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"nomkl\"\u001b[0m,\r\n",
      "                  \u001b[32m\"tensorflow>2\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"papers\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"pip_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"torchvision==0.15.2\"\u001b[0m,\r\n",
      "                  \u001b[32m\"mlflow-skinny==2.5.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"albumentations==1.3.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"scikit-image==0.21.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"chardet==5.2.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"segmentation_models.pytorch==0.2.*\"\u001b[0m,\r\n",
      "                  \u001b[32m\"werkzeug==2.2.3\"\u001b[0m,\r\n",
      "                  \u001b[32m\"torch==2.0.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"hyperspy==1.7.5\"\u001b[0m,\r\n",
      "                  \u001b[32m\"pandas==2.0.3\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"repositories\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"python_version\"\u001b[0m: \u001b[32m\"3.10.12\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"short_name\"\u001b[0m: \u001b[32m\"rtdefect_torch_3\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/5jzj-0j60\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Ward, Logan\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m\r\n",
      "              \u001b[1m}\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"publisher\"\u001b[0m: \u001b[32m\"Garden-AI\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[32m\"Ward, Logan\"\u001b[0m,\r\n",
      "              \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"pipeline_ids\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[32m\"10.23677/b246-hj14\"\u001b[0m,\r\n",
      "              \u001b[32m\"10.23677/xn48-pr25\"\u001b[0m,\r\n",
      "              \u001b[32m\"10.23677/5jzj-0j60\"\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[32m\"Max Tuecke\"\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/nzhf-rq49\"\u001b[0m\r\n",
      "          \u001b[1m}\u001b[0m,\r\n",
      "          \u001b[1;34m\"entry_id\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "          \u001b[1;34m\"matched_principal_sets\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "        \u001b[1m}\u001b[0m\r\n",
      "      \u001b[1m]\u001b[0m,\r\n",
      "      \u001b[1;34m\"subject\"\u001b[0m: \u001b[32m\"10.23677/nzhf-rq49\"\u001b[0m,\r\n",
      "      \u001b[1;34m\"@version\"\u001b[0m: \u001b[32m\"2019-08-27\"\u001b[0m\r\n",
      "    \u001b[1m}\u001b[0m\r\n",
      "  \u001b[1m]\u001b[0m,\r\n",
      "  \u001b[1;34m\"offset\"\u001b[0m: \u001b[1;36m0\u001b[0m,\r\n",
      "  \u001b[1;34m\"@datatype\"\u001b[0m: \u001b[32m\"GSearchResult\"\u001b[0m,\r\n",
      "  \u001b[1;34m\"total\"\u001b[0m: \u001b[1;36m1\u001b[0m,\r\n",
      "  \u001b[1;34m\"@version\"\u001b[0m: \u001b[32m\"2017-09-01\"\u001b[0m,\r\n",
      "  \u001b[1;34m\"count\"\u001b[0m: \u001b[1;36m1\u001b[0m,\r\n",
      "  \u001b[1;34m\"has_next_page\"\u001b[0m: \u001b[3;91mfalse\u001b[0m\r\n",
      "\u001b[1m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Search for the new published garden\n",
    "!garden-ai garden search --title=\"RT Defect Analysis Torch Demo Garden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85a5692f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'void_frac': 0.00034999847412109375, 'void_count': 6, 'radii': [5.322553886092406, 3.989422804014327, 4.1073621666150775, 4.029119531035698, 4.259537945889915, 4.61809077155419], 'radii_average': 4.387681184200269, 'positions': [(219.5505617977528, 711.0224719101124), (312.46, 249.16), (409.3207547169811, 441.64150943396226), (513.6862745098039, 205.58823529411765), (628.7719298245614, 150.03508771929825), (700.8955223880597, 1017.8955223880597)]}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: get and run the new published garden\n",
    "rtdefects_garden_published = client.get_published_garden(GARDEN_DOI)\n",
    "results = rtdefects_garden_published.rtdefect_torch_1(demo_input, endpoint=\"6d39d01e-2955-47b9-a1f6-50f147e650d6\")\n",
    "print(results[\"defect_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf237f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
