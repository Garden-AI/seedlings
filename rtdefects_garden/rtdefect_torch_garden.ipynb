{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ee4f4e",
   "metadata": {},
   "source": [
    "# Real Time Defect Analysis: Pytorch Seedling\n",
    "This notebook will show how to register and run the three pytorch models used in https://github.com/ivem-argonne/real-time-defect-analysis/tree/main with Garden.\n",
    "## Enviorment setup\n",
    "**This notebook is intended to be run from the provided anaconda enviorment.** Run ```conda env create -f ./environment.yml``` to create the enviorment, ```conda activate rtdefects``` to activate it, and then relaunch your Jupyter notebook from inside the enviorment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e052d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version is 3.10.*\n",
    "import sys\n",
    "assert sys.version_info[0] == 3 and sys.version_info[1] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24850e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import garden_ai\n",
    "from garden_ai import step, GardenClient\n",
    "\n",
    "import json\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from hashlib import md5\n",
    "from skimage import color, measure, morphology\n",
    "from io import BytesIO\n",
    "from time import perf_counter\n",
    "from hyperspy import io as hsio\n",
    "from scipy.stats import siegelslopes\n",
    "from scipy.interpolate import interp1d\n",
    "import albumentations as albu\n",
    "import imageio.v2 as imageio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b14a3",
   "metadata": {},
   "source": [
    "## Step 1: Register the model\n",
    "The first step is to register the model files with Garden. This model has already been registered with Garden, so you can skip this step.\n",
    "If you did want to re-register the model, run the command ```garden-ai model register short_name path_to_model_file flavor```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcfc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model uri's for the pre-registered pytorch models\n",
    "REGISTERED_MODEL_NAME_1 = \"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\" #small_voids_031023.pth\n",
    "REGISTERED_MODEL_NAME_2 = \"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\" #voids_segmentation_091321.pth\n",
    "REGISTERED_MODEL_NAME_3 = \"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\" #voids_segmentation_030323.pth\n",
    "\n",
    "TEST_INPUT_PATH = \"./data/input_image.tiff\"\n",
    "TEST_OUTPUT_MASK_PATH_1 = \"./data/torch_1_output_mask.tiff\"\n",
    "TEST_OUTPUT_MASK_PATH_2 = \"./data/torch_2_output_mask.tiff\"\n",
    "TEST_OUTPUT_MASK_PATH_3 = \"./data/torch_3_output_mask.tiff\"\n",
    "TEST_OUTPUT_DEFECT_PATH_1 = \"./data/torch_1_output_defect_results.json\"\n",
    "TEST_OUTPUT_DEFECT_PATH_2 = \"./data/torch_2_output_defect_results.json\"\n",
    "TEST_OUTPUT_DEFECT_PATH_3 = \"./data/torch_3_output_defect_results.json\"\n",
    "\n",
    "# Pre-made DOI's to register example seedling resources with\n",
    "PIPELINE_DOI_1 = \"10.23677/b246-hj14\"\n",
    "PIPELINE_DOI_2 = \"10.23677/xn48-pr25\"\n",
    "PIPELINE_DOI_3 = \"10.23677/5jzj-0j60\"\n",
    "GARDEN_DOI = \"10.23677/nzhf-rq49\"\n",
    "\n",
    "# Pipeline requirments\n",
    "PIP_REQUIREMENTS = [\"torchvision==0.15.2\", \"torch==2.0.1\", \"segmentation_models.pytorch==0.2.*\", \"pandas==2.0.3\", \"scikit-image==0.21.0\", \"chardet==5.2.0\", \"hyperspy==1.7.5\", \"werkzeug==2.2.3\", \"albumentations==1.3.1\"]\n",
    "CONDA_REQUIREMENTS = [\"tensorflow>2\", \"nomkl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98617f37",
   "metadata": {},
   "source": [
    "## Step 2: Create the pipelines\n",
    "Now that we have our models registered with Garden, we can create some pipelines to use them. A pipeline consists of any number of Python functions called steps that will be chained together during execution. Below are some example's of some possible step functions. We will construct three piplines (one for each torch model) using those steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcacfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate functions with `@step` so that we can use it to build up a pipeline\n",
    "\n",
    "#first preprocessing step for all torch models\n",
    "@step\n",
    "def preprocessing_all(\n",
    "    input_data: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    from typing import Optional, Tuple\n",
    "    from io import BytesIO\n",
    "    from skimage import color, measure, morphology\n",
    "    from skimage.transform import resize\n",
    "    import numpy as np\n",
    "    import imageio.v2 as imageio\n",
    "\n",
    "    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\n",
    "        # Convert mask to a uint8-compatible image\n",
    "        data = np.squeeze(data)\n",
    "        assert data.ndim == 2, \"Image must be grayscale\"\n",
    "        assert np.logical_and(data >= 0, data <= 1).all(), \"Image values must be between 0 and 1\"\n",
    "        data = np.array(data * 255, dtype=np.uint8)\n",
    "\n",
    "        # Convert mask to a TIFF-encoded image\n",
    "        output_img = BytesIO()\n",
    "        writer = imageio.get_writer(output_img, format='tiff', mode='i')\n",
    "        writer.append_data(data, meta={'compression': compress_type})\n",
    "        return output_img.getvalue()\n",
    "    \n",
    "    #Encode image data as tiff\n",
    "    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\n",
    "\n",
    "    # Load the TIFF file into a numpy array\n",
    "    image_gray = imageio.imread(BytesIO(encoded_image_data))\n",
    "\n",
    "    # Convert to RGB\n",
    "    image: np.ndarray = color.gray2rgb(image_gray)\n",
    "\n",
    "    # Scale to 1024x1024\n",
    "    if image.shape[:2] != (1024, 1024):\n",
    "        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\n",
    "\n",
    "    return image\n",
    "\n",
    "#second preprocessing step for the first torch model\n",
    "@step\n",
    "def preprocessing_torch_1(\n",
    "    input_data: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import torch\n",
    "    \n",
    "    MODEL_NAME = \"small_voids_031023.pth\"\n",
    "    \n",
    "    # Define the conversion from image to inputs\n",
    "    def to_tensor(x: np.ndarray, **kwargs):\n",
    "        return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "    _encoders = {\n",
    "        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\n",
    "        'voids_segmentation_030323.pth': 'efficientnet-b3',\n",
    "        'small_voids_031023.pth': 'se_resnext50_32x4d',\n",
    "    }\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    preprocess = albu.Compose(_transform)\n",
    "\n",
    "    # Perform the preprocessing\n",
    "    image = preprocess(image=input_data)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\n",
    "\n",
    "#second preprocessing step for the second torch model\n",
    "@step\n",
    "def preprocessing_torch_2(\n",
    "    input_data: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import torch\n",
    "    \n",
    "    MODEL_NAME = \"voids_segmentation_091321.pth\"\n",
    "    \n",
    "    # Define the conversion from image to inputs\n",
    "    def to_tensor(x: np.ndarray, **kwargs):\n",
    "        return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "    _encoders = {\n",
    "        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\n",
    "        'voids_segmentation_030323.pth': 'efficientnet-b3',\n",
    "        'small_voids_031023.pth': 'se_resnext50_32x4d',\n",
    "    }\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    preprocess = albu.Compose(_transform)\n",
    "\n",
    "    # Perform the preprocessing\n",
    "    image = preprocess(image=input_data)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\n",
    "\n",
    "#second preprocessing step for the third torch model\n",
    "@step\n",
    "def preprocessing_torch_3(\n",
    "    input_data: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import torch\n",
    "    \n",
    "    MODEL_NAME = \"voids_segmentation_030323.pth\"\n",
    "    \n",
    "    # Define the conversion from image to inputs\n",
    "    def to_tensor(x: np.ndarray, **kwargs):\n",
    "        return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "    _encoders = {\n",
    "        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\n",
    "        'voids_segmentation_030323.pth': 'efficientnet-b3',\n",
    "        'small_voids_031023.pth': 'se_resnext50_32x4d',\n",
    "    }\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    preprocess = albu.Compose(_transform)\n",
    "\n",
    "    # Perform the preprocessing\n",
    "    image = preprocess(image=input_data)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\n",
    "\n",
    "#run step for the first torch model\n",
    "@step\n",
    "def run_inference_model_1(\n",
    "    input_data: torch.Tensor,\n",
    "    model=garden_ai.Model(REGISTERED_MODEL_NAME_1),  # loads the registered model by name, with a `.predict()` method\n",
    ") -> torch.Tensor:\n",
    "    return model.predict(input_data)\n",
    "\n",
    "#run step for the second torch model\n",
    "@step\n",
    "def run_inference_model_2(\n",
    "    input_data: torch.Tensor,\n",
    "    model=garden_ai.Model(REGISTERED_MODEL_NAME_2),  # loads the registered model by name, with a `.predict()` method\n",
    ") -> torch.Tensor:\n",
    "    return model.predict(input_data)\n",
    "\n",
    "#run step for the third torch model\n",
    "@step\n",
    "def run_inference_model_3(\n",
    "    input_data: torch.Tensor,\n",
    "    model=garden_ai.Model(REGISTERED_MODEL_NAME_3),  # loads the registered model by name, with a `.predict()` method\n",
    ") -> torch.Tensor:\n",
    "    return model.predict(input_data)\n",
    "    \n",
    "#postprocessing for all torch models\n",
    "@step\n",
    "def postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\n",
    "    from typing import Optional, Tuple\n",
    "    import numpy as np\n",
    "    from io import BytesIO\n",
    "    from skimage import color, measure, morphology\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import albumentations as albu\n",
    "    import imageio.v2 as imageio\n",
    "    import torch\n",
    "\n",
    "    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\n",
    "        # Convert mask to a uint8-compatible image\n",
    "        data = np.squeeze(data)\n",
    "        assert data.ndim == 2, \"Image must be grayscale\"\n",
    "        assert np.logical_and(data >= 0, data <= 1).all(), \"Image values must be between 0 and 1\"\n",
    "        data = np.array(data * 255, dtype=np.uint8)\n",
    "\n",
    "        # Convert mask to a TIFF-encoded image\n",
    "        output_img = BytesIO()\n",
    "        writer = imageio.get_writer(output_img, format='tiff', mode='i')\n",
    "        writer.append_data(data, meta={'compression': compress_type})\n",
    "        return output_img.getvalue()\n",
    "\n",
    "    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\n",
    "        mask = morphology.remove_small_objects(mask, min_size=min_size)\n",
    "        mask = morphology.remove_small_holes(mask, min_size)\n",
    "        mask = morphology.binary_erosion(mask, morphology.square(1))\n",
    "        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\n",
    "\n",
    "        # Assign labels to the labeled regions\n",
    "        labels = measure.label(mask)\n",
    "        output['void_count'] = int(labels.max())\n",
    "\n",
    "        # Compute region properties\n",
    "        props = measure.regionprops(labels, mask)\n",
    "        radii = [p['equivalent_diameter'] / 2 for p in props]\n",
    "        output['radii'] = radii\n",
    "        output['radii_average'] = np.average(radii)\n",
    "        output['positions'] = [p['centroid'] for p in props]\n",
    "        return output, labels\n",
    "    \n",
    "    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # Make it into a bool array\n",
    "    segment = np.squeeze(input_data_numpy)\n",
    "    mask = segment > 0.9\n",
    "\n",
    "    # Generate the analysis results\n",
    "    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\n",
    "\n",
    "    # Convert mask to a TIFF-encoded image\n",
    "    mask_data = encode_as_tiff(mask)\n",
    "    \n",
    "    output = {\"mask\" : mask_data, \"defect_results\" : defect_results}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1288b14",
   "metadata": {},
   "source": [
    "With our steps defined, we can now make the pipeline objects. The parameter ```steps``` is a tuple giving the step functions in the desired order of execution, from left to right.  The output of the previous step will be send to the next. Note that at this point the pipeline is still a local object and has not been registered with Garden.\n",
    "\n",
    "**NOTE:** We are manually setting doi to PIPELINE_DOI which is pre-generated so the pipelines will always be registered to the same place. If you want to register a new pipeline with a new doi, just remove the argument and a new one will be generated. We will also do this when creating a new Garden later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889e304e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created pipelines\n"
     ]
    }
   ],
   "source": [
    "# Make the three pipelines using the steps we defined\n",
    "\n",
    "rtdefect_pipeline_1 = client.create_pipeline(\n",
    "    title=\"RT Defect Analysis Torch 1 Demo Pipeline\",\n",
    "    python_version=f\"{sys.version_info[0]}.{sys.version_info[1]}.{sys.version_info[2]}\",\n",
    "    pip_dependencies=PIP_REQUIREMENTS,\n",
    "    conda_dependencies=CONDA_REQUIREMENTS,\n",
    "    steps=(preprocessing_all, preprocessing_torch_1, run_inference_model_1, postprocessing_all),  # steps run in order, passing output to subsequent steps\n",
    "    authors=[\n",
    "        \"Ward, Logan\",\n",
    "    ],\n",
    "    contributors=[\"Tuecke, Max\"],\n",
    "    version=\"0.0.1\",\n",
    "    year=2023,\n",
    "    tags=[],\n",
    "    short_name=\"rtdefect_torch_1\",\n",
    "    doi=PIPELINE_DOI_1,\n",
    ")\n",
    "\n",
    "\n",
    "rtdefect_pipeline_2 = client.create_pipeline(\n",
    "    title=\"RT Defect Analysis Torch 2 Demo Pipeline\",\n",
    "    python_version=f\"{sys.version_info[0]}.{sys.version_info[1]}.{sys.version_info[2]}\",\n",
    "    pip_dependencies=PIP_REQUIREMENTS,\n",
    "    conda_dependencies=CONDA_REQUIREMENTS,\n",
    "    steps=(preprocessing_all, preprocessing_torch_2, run_inference_model_2, postprocessing_all),  # steps run in order, passing output to subsequent steps\n",
    "    authors=[\n",
    "        \"Ward, Logan\",\n",
    "    ],\n",
    "    contributors=[\"Tuecke, Max\"],\n",
    "    version=\"0.0.1\",\n",
    "    year=2023,\n",
    "    tags=[],\n",
    "    short_name=\"rtdefect_torch_2\",\n",
    "    doi=PIPELINE_DOI_2,\n",
    ")\n",
    "\n",
    "rtdefect_pipeline_3 = client.create_pipeline(\n",
    "    title=\"RT Defect Analysis Torch 3 Demo Pipeline\",\n",
    "    python_version=f\"{sys.version_info[0]}.{sys.version_info[1]}.{sys.version_info[2]}\",\n",
    "    pip_dependencies=PIP_REQUIREMENTS,\n",
    "    conda_dependencies=CONDA_REQUIREMENTS,\n",
    "    steps=(preprocessing_all, preprocessing_torch_3, run_inference_model_3, postprocessing_all),  # steps run in order, passing output to subsequent steps\n",
    "    authors=[\n",
    "        \"Ward, Logan\",\n",
    "    ],\n",
    "    contributors=[\"Tuecke, Max\"],\n",
    "    version=\"0.0.1\",\n",
    "    year=2023,\n",
    "    tags=[],\n",
    "    short_name=\"rtdefect_torch_3\",\n",
    "    doi=PIPELINE_DOI_3,\n",
    ")\n",
    "\n",
    "print(\"Created pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d39a3",
   "metadata": {},
   "source": [
    "## Step 3: Register the pipeline\n",
    "Now that we have our pipelines defined, it is time to register them with Garden. Normally, registering a new pipeline creates a new container using the pipeline dependencies and then uploads the pipeline to Garden. However, a container for these pipelines already exists, so we will just manually set container_id and skip the time-consuming process of building a new container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4925c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container ID: d81c62ed-9df6-4ecf-a5a3-4e7e666c2917\n",
      "Registered pipeline '10.23677/b246-hj14'!\n",
      "Registered pipeline '10.23677/xn48-pr25'!\n",
      "Registered pipeline '10.23677/5jzj-0j60'!\n"
     ]
    }
   ],
   "source": [
    "#container_id = client.build_container(rtdefect_pipeline_1) # if you want to build a fresh container instead\n",
    "\n",
    "container_id = \"d81c62ed-9df6-4ecf-a5a3-4e7e666c2917\"\n",
    "print(f\"Container ID: {container_id}\")\n",
    "\n",
    "rtdefect_pipeline_1.container_uuid = container_id\n",
    "rtdefect_pipeline_2.container_uuid = container_id\n",
    "rtdefect_pipeline_3.container_uuid = container_id\n",
    "\n",
    "client.register_pipeline(rtdefect_pipeline_1, container_id)\n",
    "print(f\"Registered pipeline '{rtdefect_pipeline_1.doi}'!\")\n",
    "\n",
    "client.register_pipeline(rtdefect_pipeline_2, container_id)\n",
    "print(f\"Registered pipeline '{rtdefect_pipeline_2.doi}'!\")\n",
    "\n",
    "client.register_pipeline(rtdefect_pipeline_3, container_id)\n",
    "print(f\"Registered pipeline '{rtdefect_pipeline_3.doi}'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d5a4d",
   "metadata": {},
   "source": [
    "## Sanity check: Pipeline execution\n",
    "At this point, you should now have three newly registered piplines with Garden. To confirm that the pipelines exists and are working, let's quickly fetch them from Garden using their DOIs and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717bc553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:hyperspy.io:If this file format is supported, please report this error to the HyperSpy developers.\n"
     ]
    }
   ],
   "source": [
    "# rtdefects input image loader - takes a path to an input image and loads it into np.ndarray for the pipeline.\n",
    "def load_rtdefects_input(path: Path) -> np.ndarray:\n",
    "    # Step 1: attempt to read it with imageio\n",
    "    load_functions = [\n",
    "        imageio.imread,\n",
    "        lambda x: hsio.load(x).data\n",
    "    ]\n",
    "    data = None\n",
    "    for function in load_functions:\n",
    "        try:\n",
    "            data: np.ndarray = function(path)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    if data is None:\n",
    "        raise ValueError(f'Failed to load image from {path}')\n",
    "\n",
    "    # Standardize the format\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    data = np.squeeze(data)\n",
    "    if data.ndim == 3:\n",
    "        data = color.rgb2gray(data)\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    return data\n",
    "\n",
    "demo_input = load_rtdefects_input(TEST_INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4760e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline 1 remote execution.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline 2 remote execution.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline 3 remote execution.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! All pipelines executed with correct results.\n"
     ]
    }
   ],
   "source": [
    "# results we want to reproduce:\n",
    "with open(TEST_OUTPUT_MASK_PATH_1, \"rb\") as img:\n",
    "\texpected_mask_1 = img.read()\n",
    "expected_defects_1 = json.load(open(TEST_OUTPUT_DEFECT_PATH_1))\n",
    "\n",
    "with open(TEST_OUTPUT_MASK_PATH_2, \"rb\") as img:\n",
    "\texpected_mask_2 = img.read()\n",
    "expected_defects_2 = json.load(open(TEST_OUTPUT_DEFECT_PATH_2))\n",
    "\n",
    "with open(TEST_OUTPUT_MASK_PATH_3, \"rb\") as img:\n",
    "\texpected_mask_3 = img.read()\n",
    "expected_defects_3 = json.load(open(TEST_OUTPUT_DEFECT_PATH_3))\n",
    "\n",
    "\n",
    "# fetch the new pipelines from Garden with the DOI\n",
    "# Note: these pipelines are not discoverable yet as none of them have been added to a Garden\n",
    "\n",
    "print(\"Starting pipeline 1 remote execution.\")\n",
    "rtdefect_remote_1 = client.get_registered_pipeline(PIPELINE_DOI_1)\n",
    "results_1 = rtdefect_remote_1(\n",
    "    demo_input,\n",
    "    endpoint=\"86a47061-f3d9-44f0-90dc-56ddc642c000\",  # execute on Globus Compute endpoint of choice\n",
    ")\n",
    "assert results_1[\"mask\"] == expected_mask_1\n",
    "assert json.loads(json.dumps(results_1[\"defect_results\"])) == expected_defects_1 #use json here to change tuples to lists, makes result same format as expected\n",
    "\n",
    "\n",
    "print(\"Starting pipeline 2 remote execution.\")\n",
    "rtdefect_remote_2 = client.get_registered_pipeline(PIPELINE_DOI_2)\n",
    "results_2 = rtdefect_remote_2(\n",
    "    demo_input,\n",
    "    endpoint=\"86a47061-f3d9-44f0-90dc-56ddc642c000\",  # execute on Globus Compute endpoint of choice\n",
    ")\n",
    "assert results_2[\"mask\"] == expected_mask_2\n",
    "assert json.loads(json.dumps(results_2[\"defect_results\"])) == expected_defects_2 #use json here to change tuples to lists, makes result same format as expected\n",
    "\n",
    "\n",
    "print(\"Starting pipeline 3 remote execution.\")\n",
    "rtdefect_remote_3 = client.get_registered_pipeline(PIPELINE_DOI_3)\n",
    "results_3 = rtdefect_remote_3(\n",
    "    demo_input,\n",
    "    endpoint=\"86a47061-f3d9-44f0-90dc-56ddc642c000\",  # execute on Globus Compute endpoint of choice\n",
    ")\n",
    "assert results_3[\"mask\"] == expected_mask_3\n",
    "assert json.loads(json.dumps(results_3[\"defect_results\"])) == expected_defects_3 #use json here to change tuples to lists, makes result same format as expected\n",
    "\n",
    "\n",
    "print(\"Done! All pipelines executed with correct results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f458ed",
   "metadata": {},
   "source": [
    "## Step 4: Create and publish a new Garden\n",
    "The final step is to add the newly registered pipelines to a Garden and publish it. First, create a new Garden and add the pipline's DOIs to its pipeline_ids list.\n",
    "**Note:** A Garden can have multiple pipelines and models associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a0b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdefect_garden_torch = client.create_garden(\n",
    "    title=\"RT Defect Analysis Torch Demo Garden\",\n",
    "    authors=[\"Max Tuecke\"],\n",
    "    description=\"Recreates the RT Defect Analysis pytorch model from https://github.com/ivem-argonne/real-time-defect-analysis/tree/main\",\n",
    "    doi=GARDEN_DOI,\n",
    ")\n",
    "# include the pipelines by just its DOI:\n",
    "rtdefect_garden_torch.pipeline_ids += [PIPELINE_DOI_1]\n",
    "rtdefect_garden_torch.pipeline_ids += [PIPELINE_DOI_2]\n",
    "rtdefect_garden_torch.pipeline_ids += [PIPELINE_DOI_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f696a",
   "metadata": {},
   "source": [
    "Now all thats left is to publish the new Garden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d42085de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish our new garden, making it (and its pipelines) discoverable by other garden users\n",
    "client.publish_garden_metadata(rtdefect_garden_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1aab00",
   "metadata": {},
   "source": [
    "## Sanity check: Garden search and execution\n",
    "Let's make sure that our new Garden is now published by searching for it with the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fef59fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m{\u001b[0m\r\n",
      "  \u001b[1;34m\"gmeta\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "    \u001b[1m{\u001b[0m\r\n",
      "      \u001b[1;34m\"@datatype\"\u001b[0m: \u001b[32m\"GMetaResult\"\u001b[0m,\r\n",
      "      \u001b[1;34m\"entries\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "        \u001b[1m{\u001b[0m\r\n",
      "          \u001b[1;34m\"content\"\u001b[0m: \u001b[1m{\u001b[0m\r\n",
      "            \u001b[1;34m\"pipeline_aliases\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\r\n",
      "            \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"description\"\u001b[0m: \u001b[32m\"Recreates the RT Defect Analysis pytorch model from https://github.com/ivem-argonne/real-time-defect-analysis/tree/main\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"language\"\u001b[0m: \u001b[32m\"en\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch Demo Garden\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"pipelines\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[1m{\u001b[0m\r\n",
      "                \u001b[1;34m\"models\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"flavor\"\u001b[0m: \u001b[32m\"pytorch\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"user_email\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"full_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_name\"\u001b[0m: \u001b[32m\"rtdefect-torch-model-1-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"serialize_type\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"mlflow_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com-rtdefect-torch-model-1-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"dataset\"\u001b[0m: \u001b[3;35mnull\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                \u001b[1;34m\"func_uuid\"\u001b[0m: \u001b[32m\"434a28d9-2e90-4b2d-80df-1ecfcc748f9b\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch 1 Demo Pipeline\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"steps\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_all: (input_data: numpy.ndarray) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_all(\\n    input_data: np.ndarray,\\n) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    from skimage.transform import resize\\n    import numpy as np\\n    import imageio.v2 as imageio\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n    \\n    #Encode image data as tiff\\n    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\\n\\n    # Load the TIFF file into a numpy array\\n    image_gray = imageio.imread(BytesIO(encoded_image_data))\\n\\n    # Convert to RGB\\n    image: np.ndarray = color.gray2rgb(image_gray)\\n\\n    # Scale to 1024x1024\\n    if image.shape[:2] != (1024, 1024):\\n        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\\n\\n    return image\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_torch_1: (input_data: numpy.ndarray) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_torch_1(\\n    input_data: np.ndarray,\\n) -> torch.Tensor:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import torch\\n    \\n    MODEL_NAME = \\\"small_voids_031023.pth\\\"\\n    \\n    # Define the conversion from image to inputs\\n    def to_tensor(x: np.ndarray, **kwargs):\\n        return x.transpose(2, 0, 1).astype('float32')\\n\\n    _encoders = {\\n        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\\n        'voids_segmentation_030323.pth': 'efficientnet-b3',\\n        'small_voids_031023.pth': 'se_resnext50_32x4d',\\n    }\\n    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\\n    \\n    _transform = [\\n        albu.Lambda(image=preprocessing_fn),\\n        albu.Lambda(image=to_tensor),\\n    ]\\n    preprocess = albu.Compose(_transform)\\n\\n    # Perform the preprocessing\\n    image = preprocess(image=input_data)\\n\\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_torch_1\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"run_inference_model_1: (input_data: torch.Tensor, model=<__main__._Model object at 0x7f8e6ef18340>) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                      \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-1-seedling\"\u001b[0m\r\n",
      "                    \u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef run_inference_model_1(\\n    input_data: torch.Tensor,\\n    model=garden_ai.Model(REGISTERED_MODEL_NAME_1),  # loads the registered model by name, with a `.predict()` method\\n) -> torch.Tensor:\\n    return model.predict(input_data)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"run_inference_model_1\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"postprocessing_all: (input_data: torch.Tensor) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import imageio.v2 as imageio\\n    import torch\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n\\n    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\\n        mask = morphology.remove_small_objects(mask, min_size=min_size)\\n        mask = morphology.remove_small_holes(mask, min_size)\\n        mask = morphology.binary_erosion(mask, morphology.square(1))\\n        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\\n\\n        # Assign labels to the labeled regions\\n        labels = measure.label(mask)\\n        output['void_count'] = int(labels.max())\\n\\n        # Compute region properties\\n        props = measure.regionprops(labels, mask)\\n        radii = [p['equivalent_diameter'] / 2 for p in props]\\n        output['radii'] = radii\\n        output['radii_average'] = np.average(radii)\\n        output['positions'] = [p['centroid'] for p in props]\\n        return output, labels\\n    \\n    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\\n\\n    # Make it into a bool array\\n    segment = np.squeeze(input_data_numpy)\\n    mask = segment > 0.9\\n\\n    # Generate the analysis results\\n    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\\n\\n    # Convert mask to a TIFF-encoded image\\n    mask_data = encode_as_tiff(mask)\\n    \\n    output = {\\\"mask\\\" : mask_data, \\\"defect_results\\\" : defect_results}\\n    \\n    return output\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"postprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"conda_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"nomkl\"\u001b[0m,\r\n",
      "                  \u001b[32m\"tensorflow>2\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"papers\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"pip_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"torchvision==0.15.2\"\u001b[0m,\r\n",
      "                  \u001b[32m\"mlflow-skinny==2.5.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"albumentations==1.3.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"scikit-image==0.21.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"chardet==5.2.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"segmentation_models.pytorch==0.2.*\"\u001b[0m,\r\n",
      "                  \u001b[32m\"werkzeug==2.2.3\"\u001b[0m,\r\n",
      "                  \u001b[32m\"torch==2.0.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"hyperspy==1.7.5\"\u001b[0m,\r\n",
      "                  \u001b[32m\"pandas==2.0.3\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"repositories\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"python_version\"\u001b[0m: \u001b[32m\"3.10.12\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"short_name\"\u001b[0m: \u001b[32m\"rtdefect_torch_1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/b246-hj14\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Ward, Logan\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m\r\n",
      "              \u001b[1m}\u001b[0m,\r\n",
      "              \u001b[1m{\u001b[0m\r\n",
      "                \u001b[1;34m\"models\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"flavor\"\u001b[0m: \u001b[32m\"pytorch\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"user_email\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"full_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_name\"\u001b[0m: \u001b[32m\"rtdefect-torch-model-2-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"serialize_type\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"mlflow_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com-rtdefect-torch-model-2-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"dataset\"\u001b[0m: \u001b[3;35mnull\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                \u001b[1;34m\"func_uuid\"\u001b[0m: \u001b[32m\"39ffb237-3e6c-498e-a798-de8e55190767\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch 2 Demo Pipeline\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"steps\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_all: (input_data: numpy.ndarray) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_all(\\n    input_data: np.ndarray,\\n) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    from skimage.transform import resize\\n    import numpy as np\\n    import imageio.v2 as imageio\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n    \\n    #Encode image data as tiff\\n    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\\n\\n    # Load the TIFF file into a numpy array\\n    image_gray = imageio.imread(BytesIO(encoded_image_data))\\n\\n    # Convert to RGB\\n    image: np.ndarray = color.gray2rgb(image_gray)\\n\\n    # Scale to 1024x1024\\n    if image.shape[:2] != (1024, 1024):\\n        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\\n\\n    return image\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r",
      "\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_torch_2: (input_data: numpy.ndarray) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_torch_2(\\n    input_data: np.ndarray,\\n) -> torch.Tensor:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import torch\\n    \\n    MODEL_NAME = \\\"voids_segmentation_091321.pth\\\"\\n    \\n    # Define the conversion from image to inputs\\n    def to_tensor(x: np.ndarray, **kwargs):\\n        return x.transpose(2, 0, 1).astype('float32')\\n\\n    _encoders = {\\n        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\\n        'voids_segmentation_030323.pth': 'efficientnet-b3',\\n        'small_voids_031023.pth': 'se_resnext50_32x4d',\\n    }\\n    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\\n    \\n    _transform = [\\n        albu.Lambda(image=preprocessing_fn),\\n        albu.Lambda(image=to_tensor),\\n    ]\\n    preprocess = albu.Compose(_transform)\\n\\n    # Perform the preprocessing\\n    image = preprocess(image=input_data)\\n\\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \\n    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_torch_2\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"run_inference_model_2: (input_data: torch.Tensor, model=<__main__._Model object at 0x7f8e80a97040>) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                      \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-2-seedling\"\u001b[0m\r\n",
      "                    \u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef run_inference_model_2(\\n    input_data: torch.Tensor,\\n    model=garden_ai.Model(REGISTERED_MODEL_NAME_2),  # loads the registered model by name, with a `.predict()` method\\n) -> torch.Tensor:\\n    return model.predict(input_data)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"run_inference_model_2\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"postprocessing_all: (input_data: torch.Tensor) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import imageio.v2 as imageio\\n    import torch\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n\\n    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\\n        mask = morphology.remove_small_objects(mask, min_size=min_size)\\n        mask = morphology.remove_small_holes(mask, min_size)\\n        mask = morphology.binary_erosion(mask, morphology.square(1))\\n        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\\n\\n        # Assign labels to the labeled regions\\n        labels = measure.label(mask)\\n        output['void_count'] = int(labels.max())\\n\\n        # Compute region properties\\n        props = measure.regionprops(labels, mask)\\n        radii = [p['equivalent_diameter'] / 2 for p in props]\\n        output['radii'] = radii\\n        output['radii_average'] = np.average(radii)\\n        output['positions'] = [p['centroid'] for p in props]\\n        return output, labels\\n    \\n    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\\n\\n    # Make it into a bool array\\n    segment = np.squeeze(input_data_numpy)\\n    mask = segment > 0.9\\n\\n    # Generate the analysis results\\n    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\\n\\n    # Convert mask to a TIFF-encoded image\\n    mask_data = encode_as_tiff(mask)\\n    \\n    output = {\\\"mask\\\" : mask_data, \\\"defect_results\\\" : defect_results}\\n    \\n    return output\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"postprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"conda_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"nomkl\"\u001b[0m,\r\n",
      "                  \u001b[32m\"tensorflow>2\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"papers\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"pip_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"torchvision==0.15.2\"\u001b[0m,\r\n",
      "                  \u001b[32m\"mlflow-skinny==2.5.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"albumentations==1.3.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"scikit-image==0.21.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"chardet==5.2.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"segmentation_models.pytorch==0.2.*\"\u001b[0m,\r\n",
      "                  \u001b[32m\"werkzeug==2.2.3\"\u001b[0m,\r\n",
      "                  \u001b[32m\"torch==2.0.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"hyperspy==1.7.5\"\u001b[0m,\r\n",
      "                  \u001b[32m\"pandas==2.0.3\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"repositories\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"python_version\"\u001b[0m: \u001b[32m\"3.10.12\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"short_name\"\u001b[0m: \u001b[32m\"rtdefect_torch_2\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/xn48-pr25\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Ward, Logan\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m\r\n",
      "              \u001b[1m}\u001b[0m,\r\n",
      "              \u001b[1m{\u001b[0m\r\n",
      "                \u001b[1;34m\"models\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"flavor\"\u001b[0m: \u001b[32m\"pytorch\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"user_email\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"full_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_name\"\u001b[0m: \u001b[32m\"rtdefect-torch-model-3-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"serialize_type\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"mlflow_name\"\u001b[0m: \u001b[32m\"maxtuecke@gmail.com-rtdefect-torch-model-3-seedling\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"dataset\"\u001b[0m: \u001b[3;35mnull\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"year\"\u001b[0m: \u001b[32m\"2023\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                \u001b[1;34m\"func_uuid\"\u001b[0m: \u001b[32m\"4e82243b-1394-491a-b31c-92c279e1f4b5\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"RT Defect Analysis Torch 3 Demo Pipeline\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"steps\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_all: (input_data: numpy.ndarray) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_all(\\n    input_data: np.ndarray,\\n) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    from skimage.transform import resize\\n    import numpy as np\\n    import imageio.v2 as imageio\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n    \\n    #Encode image data as tiff\\n    encoded_image_data = encode_as_tiff(input_data, compress_type=5)\\n\\n    # Load the TIFF file into a numpy array\\n    image_gray = imageio.imread(BytesIO(encoded_image_data))\\n\\n    # Convert to RGB\\n    image: np.ndarray = color.gray2rgb(image_gray)\\n\\n    # Scale to 1024x1024\\n    if image.shape[:2] != (1024, 1024):\\n        image = resize(image, output_shape=(1024, 1024), anti_aliasing=True)\\n\\n    return image\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'numpy.ndarray'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"preprocessing_torch_3: (input_data: numpy.ndarray) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef preprocessing_torch_3(\\n    input_data: np.ndarray,\\n) -> torch.Tensor:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import torch\\n    \\n    MODEL_NAME = \\\"voids_segmentation_030323.pth\\\"\\n    \\n    # Define the conversion from image to inputs\\n    def to_tensor(x: np.ndarray, **kwargs):\\n        return x.transpose(2, 0, 1).astype('float32')\\n\\n    _encoders = {\\n        'voids_segmentation_091321.pth': 'se_resnext50_32x4d',\\n        'voids_segmentation_030323.pth': 'efficientnet-b3',\\n        'small_voids_031023.pth': 'se_resnext50_32x4d',\\n    }\\n    preprocessing_fn = smp.encoders.get_preprocessing_fn(_encoders[MODEL_NAME])\\n    \\n    _transform = [\\n        albu.Lambda(image=preprocessing_fn),\\n        albu.Lambda(image=to_tensor),\\n    ]\\n    preprocess = albu.Compose(_transform)\\n\\n    # Perform the preprocessing\\n    image = preprocess(image=input_data)\\n\\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \\n    return torch.from_numpy(image['image']).to(device).unsqueeze(0)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"preprocessing_torch_3\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"run_inference_model_3: (input_data: torch.Tensor, model=<__main__._Model object at 0x7f8e80a97580>) -> torch.Tensor\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                      \u001b[32m\"maxtuecke@gmail.com/rtdefect-torch-model-3-seedling\"\u001b[0m\r\n",
      "                    \u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'torch.Tensor'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef run_inference_model_3(\\n    input_data: torch.Tensor,\\n    model=garden_ai.Model(REGISTERED_MODEL_NAME_3),  # loads the registered model by name, with a `.predict()` method\\n) -> torch.Tensor:\\n    return model.predict(input_data)\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"run_inference_model_3\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m,\r\n",
      "                  \u001b[1m{\u001b[0m\r\n",
      "                    \u001b[1;34m\"input_info\"\u001b[0m: \u001b[32m\"{'input_data': <class 'torch.Tensor'>}\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"func\"\u001b[0m: \u001b[32m\"postprocessing_all: (input_data: torch.Tensor) -> numpy.ndarray\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"description\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "                    \u001b[1;34m\"model_full_names\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                    \u001b[1;34m\"output_info\"\u001b[0m: \u001b[32m\"return: <class 'numpy.ndarray'>\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"source\"\u001b[0m: \u001b[32m\"@step\\ndef postprocessing_all(input_data: torch.Tensor) -> np.ndarray:\\n    from typing import Optional, Tuple\\n    import numpy as np\\n    from io import BytesIO\\n    from skimage import color, measure, morphology\\n    import segmentation_models_pytorch as smp\\n    import albumentations as albu\\n    import imageio.v2 as imageio\\n    import torch\\n\\n    def encode_as_tiff(data: np.ndarray, compress_type: int = 5) -> bytes:\\n        # Convert mask to a uint8-compatible image\\n        data = np.squeeze(data)\\n        assert data.ndim == 2, \\\"Image must be grayscale\\\"\\n        assert np.logical_and(data >= 0, data <= 1).all(), \\\"Image values must be between 0 and 1\\\"\\n        data = np.array(data * 255, dtype=np.uint8)\\n\\n        # Convert mask to a TIFF-encoded image\\n        output_img = BytesIO()\\n        writer = imageio.get_writer(output_img, format='tiff', mode='i')\\n        writer.append_data(data, meta={'compression': compress_type})\\n        return output_img.getvalue()\\n\\n    def analyze_defects(mask: np.ndarray, min_size: int = 50) -> Tuple[dict, np.ndarray]:\\n        mask = morphology.remove_small_objects(mask, min_size=min_size)\\n        mask = morphology.remove_small_holes(mask, min_size)\\n        mask = morphology.binary_erosion(mask, morphology.square(1))\\n        output = {'void_frac': mask.sum() / (mask.shape[0] * mask.shape[1])}\\n\\n        # Assign labels to the labeled regions\\n        labels = measure.label(mask)\\n        output['void_count'] = int(labels.max())\\n\\n        # Compute region properties\\n        props = measure.regionprops(labels, mask)\\n        radii = [p['equivalent_diameter'] / 2 for p in props]\\n        output['radii'] = radii\\n        output['radii_average'] = np.average(radii)\\n        output['positions'] = [p['centroid'] for p in props]\\n        return output, labels\\n    \\n    input_data_numpy = input_data.squeeze().cpu().detach().numpy()\\n\\n    # Make it into a bool array\\n    segment = np.squeeze(input_data_numpy)\\n    mask = segment > 0.9\\n\\n    # Generate the analysis results\\n    defect_results, _ = analyze_defects(mask)  # Discard the labeled output\\n\\n    # Convert mask to a TIFF-encoded image\\n    mask_data = encode_as_tiff(mask)\\n    \\n    output = {\\\"mask\\\" : mask_data, \\\"defect_results\\\" : defect_results}\\n    \\n    return output\\n\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"title\"\u001b[0m: \u001b[32m\"postprocessing_all\"\u001b[0m,\r\n",
      "                    \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "                  \u001b[1m}\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"version\"\u001b[0m: \u001b[32m\"0.0.1\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"conda_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"nomkl\"\u001b[0m,\r\n",
      "                  \u001b[32m\"tensorflow>2\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"papers\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"tags\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"pip_dependencies\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"torchvision==0.15.2\"\u001b[0m,\r\n",
      "                  \u001b[32m\"mlflow-skinny==2.5.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"albumentations==1.3.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"scikit-image==0.21.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"chardet==5.2.0\"\u001b[0m,\r\n",
      "                  \u001b[32m\"segmentation_models.pytorch==0.2.*\"\u001b[0m,\r\n",
      "                  \u001b[32m\"werkzeug==2.2.3\"\u001b[0m,\r\n",
      "                  \u001b[32m\"torch==2.0.1\"\u001b[0m,\r\n",
      "                  \u001b[32m\"hyperspy==1.7.5\"\u001b[0m,\r\n",
      "                  \u001b[32m\"pandas==2.0.3\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"repositories\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"python_version\"\u001b[0m: \u001b[32m\"3.10.12\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"short_name\"\u001b[0m: \u001b[32m\"rtdefect_torch_3\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m,\r\n",
      "                \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/5jzj-0j60\"\u001b[0m,\r\n",
      "                \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "                  \u001b[32m\"Ward, Logan\"\u001b[0m\r\n",
      "                \u001b[1m]\u001b[0m\r\n",
      "              \u001b[1m}\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"publisher\"\u001b[0m: \u001b[32m\"Garden-AI\"\u001b[0m,\r\n",
      "            \u001b[1;34m\"contributors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[32m\"Ward, Logan\"\u001b[0m,\r\n",
      "              \u001b[32m\"Tuecke, Max\"\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"pipeline_ids\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[32m\"10.23677/b246-hj14\"\u001b[0m,\r\n",
      "              \u001b[32m\"10.23677/xn48-pr25\"\u001b[0m,\r\n",
      "              \u001b[32m\"10.23677/5jzj-0j60\"\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\r\n",
      "              \u001b[32m\"Max Tuecke\"\u001b[0m\r\n",
      "            \u001b[1m]\u001b[0m,\r\n",
      "            \u001b[1;34m\"doi\"\u001b[0m: \u001b[32m\"10.23677/nzhf-rq49\"\u001b[0m\r\n",
      "          \u001b[1m}\u001b[0m,\r\n",
      "          \u001b[1;34m\"entry_id\"\u001b[0m: \u001b[3;35mnull\u001b[0m,\r\n",
      "          \u001b[1;34m\"matched_principal_sets\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\r\n",
      "        \u001b[1m}\u001b[0m\r\n",
      "      \u001b[1m]\u001b[0m,\r\n",
      "      \u001b[1;34m\"subject\"\u001b[0m: \u001b[32m\"10.23677/nzhf-rq49\"\u001b[0m,\r\n",
      "      \u001b[1;34m\"@version\"\u001b[0m: \u001b[32m\"2019-08-27\"\u001b[0m\r\n",
      "    \u001b[1m}\u001b[0m\r\n",
      "  \u001b[1m]\u001b[0m,\r\n",
      "  \u001b[1;34m\"offset\"\u001b[0m: \u001b[1;36m0\u001b[0m,\r\n",
      "  \u001b[1;34m\"@datatype\"\u001b[0m: \u001b[32m\"GSearchResult\"\u001b[0m,\r\n",
      "  \u001b[1;34m\"total\"\u001b[0m: \u001b[1;36m1\u001b[0m,\r\n",
      "  \u001b[1;34m\"@version\"\u001b[0m: \u001b[32m\"2017-09-01\"\u001b[0m,\r\n",
      "  \u001b[1;34m\"count\"\u001b[0m: \u001b[1;36m1\u001b[0m,\r\n",
      "  \u001b[1;34m\"has_next_page\"\u001b[0m: \u001b[3;91mfalse\u001b[0m\r\n",
      "\u001b[1m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!garden-ai garden search --title=\"RT Defect Analysis Torch Demo Garden\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d388c",
   "metadata": {},
   "source": [
    "Finally, let's make sure we can run the pipline in the new Garden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85a5692f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'void_frac': 0.00034999847412109375, 'void_count': 6, 'radii': [5.322553886092406, 3.989422804014327, 4.1073621666150775, 4.029119531035698, 4.259537945889915, 4.61809077155419], 'radii_average': 4.387681184200269, 'positions': [(219.5505617977528, 711.0224719101124), (312.46, 249.16), (409.3207547169811, 441.64150943396226), (513.6862745098039, 205.58823529411765), (628.7719298245614, 150.03508771929825), (700.8955223880597, 1017.8955223880597)]}\n"
     ]
    }
   ],
   "source": [
    "# Get the newly published Garden with its DOI\n",
    "rtdefects_garden_published = client.get_published_garden(GARDEN_DOI)\n",
    "\n",
    "# Run a pipeline by calling the pipelines' short_name\n",
    "results = rtdefects_garden_published.rtdefect_torch_1(demo_input, endpoint=\"86a47061-f3d9-44f0-90dc-56ddc642c000\")\n",
    "print(results[\"defect_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf237f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
